<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Amazon Illegal Mining Detection | Penny Pan</title>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Amazon Illegal Mining Detection | Penny Pan</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Amazon Illegal Mining Detection" />
<meta name="author" content="penny" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive practice has led to significant habitat destruction, biodiversity loss, and contributes to climate change. A key factor exacerbating this devastation is the sharp increase in illegal gold mining activities observed in the region over the past decade. Motivated by the soaring global gold price, individuals have engaged in unlawful mining practices within the Amazon rainforest, further exacerbating the environmental crisis." />
<meta property="og:description" content="The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive practice has led to significant habitat destruction, biodiversity loss, and contributes to climate change. A key factor exacerbating this devastation is the sharp increase in illegal gold mining activities observed in the region over the past decade. Motivated by the soaring global gold price, individuals have engaged in unlawful mining practices within the Amazon rainforest, further exacerbating the environmental crisis." />
<link rel="canonical" href="http://localhost:4000/illegal_mining/" />
<meta property="og:url" content="http://localhost:4000/illegal_mining/" />
<meta property="og:site_name" content="Penny Pan" />
<meta property="og:image" content="http://localhost:4000/assets/images/1-mining.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-18T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/assets/images/1-mining.jpeg" />
<meta property="twitter:title" content="Amazon Illegal Mining Detection" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"penny"},"dateModified":"2019-01-18T00:00:00-05:00","datePublished":"2019-01-18T00:00:00-05:00","description":"The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive practice has led to significant habitat destruction, biodiversity loss, and contributes to climate change. A key factor exacerbating this devastation is the sharp increase in illegal gold mining activities observed in the region over the past decade. Motivated by the soaring global gold price, individuals have engaged in unlawful mining practices within the Amazon rainforest, further exacerbating the environmental crisis.","headline":"Amazon Illegal Mining Detection","image":"http://localhost:4000/assets/images/1-mining.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/illegal_mining/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"penny"},"url":"http://localhost:4000/illegal_mining/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Penny Pan</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": ""
    }, {
    "id": 2,
    "url": "http://localhost:4000/author-penny.html",
    "title": "Penny",
    "body": "                    {{page. title}}:         {{ site. authors. penny. site }}         {{ site. authors. penny. bio }}                                   Posts by {{page. title}}:       {% assign posts = site. posts | where: author , penny  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 3,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "{{page. title}}:     {% for author in site. authors %}                                         {{ author[1]. name }} :       (View Posts)      {{ author[1]. bio }}                                           {% endfor %}  "
    }, {
    "id": 4,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 5,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 6,
    "url": "http://localhost:4000/",
    "title": "Penny's Blog",
    "body": "{% if page. url ==  /  %}    {% assign latest_post = site. posts[0] %}        &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               {{ latest_post. date | date: '%b %d, %Y' }}                      {%- assign second_post = site. posts[1] -%}            {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                            {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                            {{ second_post. date | date: '%b %d, %Y' }}                                {%- assign third_post = site. posts[2] -%}            {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                            {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                            {{ third_post. date | date: '%b %d, %Y' }}                                {%- assign fourth_post = site. posts[3] -%}            {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                            {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                            {{ fourth_post. date | date: '%b %d, %Y' }}                          {% for post in site. posts %}{% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More                        {% endif %}{% endfor %}{% endif %}       All Stories:     {% for post in paginator. posts %}      {% include main-loop-card. html %}    {% endfor %}               {% if paginator. total_pages &gt; 1 %}             {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 7,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 8,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags    {% for tag in site. tags %}    {{ tag[0] }}:     {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}      {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}              {% include sidebar-featured. html %}    "
    }, {
    "id": 9,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 10,
    "url": "http://localhost:4000/sm-summary-table/",
    "title": "A Guide to Interpreting statsmodels summary table",
    "body": "2023/12/23 - In the realm of data science and machine learning, understanding statistical results is crucial for making informed decisions. One of the most commonly used packages data scientist encountered daily is statsmodel. Its summary table is a great tool to gain insights to understanding the relationship between explanatory variable and response variable. In this blog post, we’ll dive into how to interpret a Statsmodels summary table and extract meaningful insights from it. import statsmodels. api as smimport statsmodels. formula. api as smfimport numpy as npimport pandasWhat is a Statsmodels Summary Table?: est = sm. OLS(y, sm. add_constant(X))res = est. fit()print(res. summary())              OLS Regression Results              ==============================================================================Dep. Variable:           y  R-squared:            0. 793Model:              OLS  Adj. R-squared:         0. 687Method:         Least Squares  F-statistic:         7. 686e+04Date:        Mon, 18 Dec 2023  Prob (F-statistic):         0Time:            09:29:24  Log-Likelihood:        -375. 30No. Observations:         85  AIC:               764. 6Df Residuals:           78  BIC:               781. 7Df Model:              6                     Covariance Type:      nonrobust                     ===============================================================================         coef  std err     t   P&gt;|t|   [0. 025   0. 975]-------------------------------------------------------------------------------Intercept   38. 6517   9. 456   4. 087   0. 000   19. 826   57. 478Region[T. E]  -15. 4278   9. 727   -1. 586   0. 117   -34. 793    3. 938Region[T. N]  -10. 0170   9. 260   -1. 082   0. 283   -28. 453    8. 419Region[T. S]  -4. 5483   7. 279   -0. 625   0. 534   -19. 039    9. 943Region[T. W]  -10. 0913   7. 196   -1. 402   0. 165   -24. 418    4. 235Literacy    -0. 1858   0. 210   -0. 886   0. 378   -0. 603    0. 232Wealth     0. 4515   0. 103   4. 390   0. 000    0. 247    0. 656==============================================================================Omnibus:            3. 049  Durbin-Watson:          1. 785Prob(Omnibus):         0. 218  Jarque-Bera (JB):        2. 694Skew:             -0. 340  Prob(JB):            0. 260Kurtosis:            2. 454  Cond. No.             371. ==============================================================================Notes :[1] Standard Errors assume that the covariance matrix of the errors is correctly specifiedWhen you fit a statistical model using Statsmodels, such as linear regression, logistic regression, or any other supported model, you typically receive a summary of the model’s results, as shown above. This summary contains various statistical metrics, including coefficients, standard errors, p-values, confidence intervals, and more, depending on the type of model you’ve fitted. The table is divided into three sections.  Section I: general fitting of the models Section II: t value and p values - coefficients Section III: the shape of the distributionSection I:    Number of observations: The number of data points used in the analysis.     Method: least square. Find the best line by minimizing the the sum of the squared errors.        Degree of freedom: number of independent variables     Covariance Type: typically nonrobust, which means there is no elimination of data to calculate the covariance between features     R-squared: One of the most important number in the table, also known as the coefficient of determination. R-squared represents the proportion of variance in the dependent variable that is explained by the independent variables. It ranges from 0 to 1, with higher values indicating a better fit. For example, 0. 793 means the model explains 79. 3% of the change in y variable \[\begin{equation} R^2 = 1-\dfrac{ \sum(y_i - \hat{y}_i)^2}{(y_i - \bar{y})^2} \end{equation}\]  y~i~ = observed values, ŷ= predicted values, ȳ = mean of the observed values     Adj. R-squared: Similar to R-squared but it penalizes the addition of unnecessary predictors. Consequently, a diminished adjusted score may indicate that some variables are not contributing to your model’s R-squared.     F-statistic and Prob (F-statistic): The F-test indicates whether your linear regression model provides a better fit than a model with all parameters equal to zero. $H_0:\beta_1=\beta_2=⋯=\beta_i=0$ A small p-value (typically less than 0. 05) indicates that the model as a whole is significant.                 Log-Likelihood 1: The natural logarithm of the likelihood. The likelihood is a function that tells the probability of observing the data given the model parameters, it is a product of probability density functions 𝑓~𝜃~.  $L(𝜃     𝑋)=∏_{𝑖=1}^𝑛𝑓_𝜃(𝑋_𝑖)$.  Log-likelihood values cannot be used alone as an index of fit because they are a function of sample size but can be used to compare the fit of different coefficients.            AIC and BIC: Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are both related to log-likelihood functions. They balance model complexity with goodness of fit, helping to prevent overfitting. Lower AIC or BIC values indicate better model fit, while taking into account the number of parameters in the model. Section II - Coefficients:    Coefficients : In a classic linear formula $y = mx+\beta$​ , coefficients is our β     Std err: The standard error is the (estimated) standard deviation of the sampling distribution of 𝛽̂ $SE = \frac{\sigma}{\sqrt{n}}$  . The standard error quantifies the uncertainty or variability in this estimated coefficient. A smaller standard error indicates higher precision, meaning that the estimated coefficient is likely closer to the true population value.     t: The t-statistic, which tests the null hypothesis that the coefficient is equal to zero. Based on that test we may decide whether x is a useful (linear) predictor of y.                 **P&gt;     t     :** Another important feature in the summary table. The p-value is associated with the t-statistic. A small p-value indicates that the coefficient is statistically significant.  A p-value of 0. 05 or lower is generally considered statistically significant.              [0. 025 0. 975]: The 95% confidence interval for the coefficient. It provides a range of plausible values for the true coefficient. Section III - Shape of distribution:    Omnibus, Prob(Omnibus): The Omnibus test measures the normality of the residuals with 0 meaning perfect, and the associated p-value (Prob(Omnibus)) indicates whether the test is statistically significant.     Durbin-Watson: It is a test to detect autocorrelations in the residuals. Its scale spans from 0 to 4, where values proximate to 2 signify an absence of autocorrelation, 0 indicates positive autocorrelation, and 4 implies negative autocorrelation. Autocorrelation occurs when the errors (residuals) of a regression model exhibit patterns with themselves at different lags, and the residuals are not independent of each other.  One common approach to visually assess autocorrelation is by plotting residual plots, as depicted in the accompanying images below.        Jarque-Bera (JB) and Prob(JB): Similar to the Omnibus test, the Jarque-Bera test evaluates the normality of the residuals. The p-value (Prob(JB)) indicates whether the test is statistically significant.     Skewness and Kurtosis: Skewness measures the asymmetry of the distribution and Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution.     Conclusion: Interpreting a Statsmodels summary table requires a solid understanding of statistical concepts and an appreciation for the nuances of the model being analyzed. By carefully examining the coefficients, p-values, confidence intervals, and diagnostic statistics provided in the summary table, you can gain valuable insights into the relationships between variables and the overall performance of the model. However, it’s essential to remember that statistical analysis is not an exact science, and interpretation should always be accompanied by critical thinking and consideration of the context in which the analysis was conducted. With practice and experience, you can become proficient in interpreting Statsmodels summary tables and using them to inform data-driven decisions. Happy modeling! Reference:       https://www. statlect. com/glossary/log-likelihood by Log-likelihoodby Marco Taboga PhD.  &#8617;    "
    }, {
    "id": 11,
    "url": "http://localhost:4000/Optiver-stock-volatility-prediction/",
    "title": "Optiver stock volatility prediction",
    "body": "2022/05/12 - Project descriptionOptiver, a globally renowned high-frequency trading company, is organizing a Kaggle competition focused on discovering optimal volatility prediction models. The objective is to forecast short-term volatility for numerous stocks spanning various sectors. With access to millions of rows of detailed financial data, participants are challenged to anticipate volatility over 10-minute intervals. The performance of the models will be assessed using actual market data gathered during the three-month evaluation period following the training phase. Project Aim Predict the realized volatility for the next 10 minutes based on historical order book and transaction data for every 10-minute interval. The definition of realized volatility is as follows, where r is the log return of stock S. \(\sigma=\sqrt{\sum_t {r_{t-1,t}}^2}\\r_{t-1,t}=log(\dfrac{S_{t2}}{S_{t1}})\)Optiver provided order book and trade data as book_train/test and train/test. The training dataset contains information from 3830 10-minute segments, which have been sampled from a total of 32767 10-minute segments across 112 stocks from 3 years historical trading data.  Sec stands for seconds_in_bucket.    book_train：order book data               time_id     sec*     bid_price1     ask_price1     bid_price2     ask_price2     bid_size1     ask_size1     bid_size2     ask_size2     stock_id                   5     0     1. 001422     1. 002301     1. 00137     1. 002353     3     226     2     100     0             5     1     1. 001422     1. 002301     1. 00137     1. 002353     3     100     2     100     0             5     5     1. 001422     1. 002301     1. 00137     1. 002405     3     100     2     100     0             Trade_train: trading data               time_id     sec*     price     size     order_count     stock_id                   5     21     1. 002301     326     12     0             5     46     1. 002778     128     4     0             5     50     1. 002818     55     1     0             5     57     1. 003155     121     5     0             5     68     1. 003646     4     1     0             train. csv：target is y_true               Stock_id     time_id     Target (σ)                   9     5     0. 007291             9     11     0. 002529             9     16     0. 003299          The main goal is to use 10 minutes of book data to predict the realized volatility of the next 10-minute window - target. When plotting bid_price1, bid_price2, ask_price1, and ask_price2, it becomes evident that throughout this 10-minute timeframe, the overarching trends of these four prices remain consistent. However, variations in price movements within specific intervals are noticeable   Figure 1 (top): Order book bid and ask prices of stock id 0 time id 5. Figure 2 (bottom): Added real trade price Let’s start with a basic understanding of order book first. It helps match the best available price in the market. Trades occur when a buyer’s bid matches a seller’s ask, leading to an agreement on the trade price.  For example, if an investor wants to purchase 20 shares of stock A, they would check the sell side of the order book to find sellers offering 221 shares at the lowest price $148. Therefore as we see in figure 2, trade price falls between bid and ask.  In total, there are 112 stock ids and we randomly split into train and test with 4:1 ratio. So the first step is to calculate the realized volatility with the data given using formula (1). Because we have two bid prices and two ask prices, we can calculate two weighted average price wap1 and wap2. Feature Engineering: Feature engineering is quite important in this competition. Future Improvements: Conclusion: Reference "
    }, {
    "id": 12,
    "url": "http://localhost:4000/Ecommerce/",
    "title": "Building a Dynamic E-commerce Website for Small Businesses",
    "body": "2021/06/06 - The ongoing COVID-19 pandemic has brought about a seismic shift in consumer behavior, driving an unprecedented surge in online shopping. With an increasing preference for online shopping over in-person experiences, small business owners find themselves presented with a unique opportunity to expand their customer base and boost revenue by establishing a robust e-commerce presence. In this blog post, we’ll explore the transformative potential of setting up a simple yet dynamic website with a database for small businesses. This strategic move not only enables businesses to adapt to the changing landscape but also empowers them to thrive in the digital world. 1. Plan and Design: Crafting a Roadmap to Success: Before diving into the development process, it’s crucial to meticulously plan and design your e-commerce platform. Small businesses need to outline their goals, target audience, and unique selling propositions. A well-thought-out plan ensures that the subsequent steps align with the business’s objectives, providing a roadmap for success. To be more specific, consider the features you want to incorporate, such as color themes, payment gateway integration, and more. Popular payment gateways like PayPal, Stripe, Braintree, and Square offer API access, allowing for seamless integration and backtesting. 2. Set up coding environment: Initiate your project by establishing a virtual environment using tools like virtualenv, helps to keep project dependencies organized and prevents conflicts between different projects. Additionally, it simplifies the process of transitioning projects to different laptops or computers and enables project reproduction across different devices. Once you have created a virtual environment, you can activate it by running the command. virtualenv -p python3 &lt;desired-path&gt;source &lt;venv&gt;/bin/activate3. Building the project: Building a shopping site involves a multifaceted approach that encompasses various aspects, from designing the front end for user interaction to implementing a robust backend and database system. The front end is what users see in their browsers, while the back end is responsible for fulfilling user requests on the server side. You can either design the layout and animations yourself using CSS and JavaScript or utilize tools like Bootstrap, which expedites the front end creation of a page. When it comes to the backend, Flask stands out as an excellent choice for beginners. Its user-friendly nature, coupled with examples readily available on platforms like GitHub, makes it an ideal starting point. The micro-framework of Flask offers flexibility, allowing businesses to tailor their web applications to meet specific project needs. You can have a website up and displaying “hello world!” within seconds by utilizing the following code. import osfrom flask import Flaskdef create_app(test_config=None):  # create and configure the app  app = Flask(__name__, instance_relative_config=True)  app. config. from_mapping(    SECRET_KEY='dev',    DATABASE=os. path. join(app. instance_path, 'flaskr. sqlite'),  )  if test_config is None:    # load the instance config, if it exists, when not testing    app. config. from_pyfile('config. py', silent=True)  else:    # load the test config if passed in    app. config. from_mapping(test_config)  # ensure the instance folder exists  try:    os. makedirs(app. instance_path)  except OSError:    pass  # a simple page that says hello  @app. route('/hello')  def hello():    return 'Hello, World!'  return appflask --app flaskr run --debugThe database is the backbone of a shopping site, storing valuable information such as product details, customer data, and transaction history. Ask yourself a few questions before start.  Product Attributes:     What attributes of the product do we need to showcase to customers?   Consider aspects like product images, descriptions, prices, and any other relevant details.     Customer Information:     What information needs to be collected from customers?   Think about user registration details, shipping addresses, and any additional data relevant to your business model.    Drawing a clear and concise database schema is a critical step in visualizing the data model and establishing relationships between different entities. A standard schema typically includes tables representing various aspects of your website. Consider the following example: In this example, the grey lines depict how tables are linked. Notice the “order_detail” table sharing a common key, “order_id,” with the “orders” table. This linkage allows for accurate tracking of inventory levels and updating product availability as items are sold. Taking the time to design a well-structured database schema sets the foundation for a robust and scalable shopping website. Below is a simple example of a Flask application with a SQL database. In this example, we’ll create a basic shopping sites with products and orders.  SQLite is used for simplicity, but you can replace it with a more robust database like PostgreSQL or MySQL based on your needs. from flask import Flask, render_template, request, redirect, url_forfrom flask_sqlalchemy import SQLAlchemyapp = Flask(__name__)app. config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///shop. db' # Use SQLite for simplicitydb = SQLAlchemy(app)# Define Product and Order modelsclass Product(db. Model):  id = db. Column(db. Integer, primary_key=True)  name = db. Column(db. String(255), nullable=False)  price = db. Column(db. Float, nullable=False)class Order(db. Model):  id = db. Column(db. Integer, primary_key=True)  product_id = db. Column(db. Integer, db. ForeignKey('product. id'), nullable=False)  quantity = db. Column(db. Integer, nullable=False)Reference: Flask-login document Flask-login github demo Organzing Flask Apps with Blueprints As the website expands, consider enhancing the user experience with additional features and functionalities. Integration of payment gateways, search functionality, product reviews, wish lists, and social media integration can significantly boost user engagement and drive sales. Flask’s modular design and scalability allow you to seamlessly incorporate these features. Online tutorials from reputable sources, such as PayPal’s official guide on integrating PayPal Checkout for online payments, and various programming tutorials on building a shopping cart using JavaScript, provide valuable resources for developers. Reference: Paypal integration JavaScript Shopping Cart Tutorial 4. Testing: Testing is a crucial phase in website development, involving various tests like unit tests, integration tests, and end-to-end tests to verify different components’ functionality. Frameworks like Pytest and unittest can be employed for testing purposes, and debugging is essential to ensure smooth website functionality. 5. Deploy: Once testing is complete, the next step is to deployment. Hosting services like Heroku, AWS, and Google Cloud Platform provide platforms for deploying websites. You will need to configure the hosting environment, set up the database and server, and deploy your code to the server. It’s also vital to consider factors like scalability, security, and availability during deployment. I used pythoneverywhere. com to host my website. It is affordable and Python friendly. With a paid account, you can host up to three web applications for $5 per month. Security is paramount when handling sensitive customer information. It is our responsibility to protect user information from potential hackers and other security threats. An essential key is to use encryption to protect the data when transmitted between client and server. First, you’ll need to obtain an SSL (Secure Sockets Layer) certificate, which verifies the identity of your website and encrypts all data transmitted between the server and the client. You can obtain a SSL certificate from a trusted certificate authority such as Let’s Encrypt (FREE !!!) Once you have the certificate, configure the web server to use HTTPS instead of HTTP. HTTPS is a secure version of the HTTP protocol that encrypts all data transmitted between the client and server. In addition to using encryption, you should also take other security measures such as regularly updating your software and plugins, using strong passwords, and implementing measures such as two-factor authentication to protect your website from potential threats. It’s also a good idea to regularly back up your website to protect against data loss in the event of a security breach or other issue. 6. Google Search: Ensuring visibility on Google search results is pivotal for the success of your website. Google Search Console provides tools for verification and indexing. Purchasing a domain name, completing DNS verification, and submitting the index request are steps to ensure visibility in Google search results.    Verify the domain on google search console. You can open the console by typing in google   site:xxx. com      Complete the DNS verification as google requested     Buy a domain name.     Edit CNAME record in the DNS manage section. Here I used goDaddy as an example.  host: wwwpoints to: yourwebsitename. com Then go back to google console, search for your url in the URL inspection section and submit the index request. Google will perform several test on the website and if passed, you will receive a confirmation email from google. In conclusion, Flask proves to be a flexible and lightweight web framework that simplifies database integration and offers built-in features for creating dynamic websites. By following a comprehensive development and deployment process, you can build a secure, feature-rich shopping site that meets user needs and stands out in the competitive online marketplace. "
    }, {
    "id": 13,
    "url": "http://localhost:4000/covid/",
    "title": "Coronavirus Data Analysis",
    "body": "2020/03/14 - Subtitle: COVID19 Spread SIR Model Simulation using Python Since the President declared a national emergency concerning the coronavirus disease, the world has been thrust into an unprecedented era marked by widespread uncertainty and profound challenges. The global response to the COVID-19 pandemic has emphasized the critical need for predictive models to understand and mitigate the spread of infectious diseases. The Susceptible-Infected-Recovered (SIR) model is commonly used in epidemiology to understand and predict the spread of infectious diseases within a population. In this context, the Susceptible-Infected-Recovered (SIR) model has emerged as a valuable tool in epidemiological research, offering insights into the dynamics of contagion within populations. Since the President declared a national emergency concerning the coronavirus disease, several months have been passed since the start of quarantine. The spread of COVID 19 seems going worse as more and more colleagues and friends are getting sick. In this time huge changes have occurred since I first started this blog in early February. My initial goal was to create a source that would collect total number of cases worldwide since the most updated information was not easily available to the public. However, the current situation has now changed, and there are more reliable resources that are documenting the spread of COVID-19 and disseminating the research in clear graphs. However, if you like to try my code, I have presented it below (output is in json format). def predict(c):	post_url =  https://covid19. who. int/page-data/region/wpro/country/cn/page-data. json 	try:		res = re. get(post_url, timeout=10)		status_code = res. status_code	except re. exceptions. ConnectionError:		status_code = 'CONNECTION ERROR'	if status_code != 200:		print ('___'*20, status_code)		return 'STATUS ERROR'		json_data = json. loads(res. json()['result']['data']['countryGroup']['data'])	cols = ['date','_']+[x['name'] for x in json_data['metrics']]	df = pd. DataFrame(data=json_data['rows'], columns=cols)	df['date'] = pd. to_datetime(df['date'],utc=True,unit='ms'). dt. date	df. drop(columns=['_'],inplace=True)	#	print (df. describe(). T) #	print (df. loc[df['Deaths']&lt;0]) # abnormal data neg daily death 	df = df. loc[df['Deaths']&gt;0]	print (df) predict(c='china') Although the situation with the coronavirus seems to be under control in China, the number of individuals infected in the US is rapidly increasing. As of March 31st 2020, the confirmed cases in the US reached around 188,530 and some officials are predicting that the number of deaths may reach up to 240,000.  Figure 1 (a). Total confirmed cases in US, China, Italy                       (b). Log scale In Figure 1, we can observe the progression of the coronavirus outbreak in three countries, each in a different phase. China has managed to control the spread of the virus, with the number of cases stabilizing after day 30. Italy’s rate of infection has slowed down and could potentially follow China’s trend (as indicated by the green line). In the US, we are still in the midst of a growing phase, but there is a glimmer of hope as further calculations suggest a possible slowdown (as indicated by the tip of the blue line). Figure 1b, which uses a log scale, demonstrates that the US and China have a higher infection rate due to their high population density. The slope of the graph in Figure 1b is calculated every 14 days using linear regression, and is obtained by breaking the data into smaller sections and taking the maximum slope. Analysis: Figure 1(a) shows that US data (blue line) probably follows an exponential trend ain the early stage, as also shown in the log Figure 1(b). Whereas, the green curve (China) looks more like a sigmoid function. So let’s first try to predict the infection population I(t) with a simple exponential model, defining I as infection population with an assumption that each patient infects A number of new people every day. This can be tested by by plotting log(y) against t - Figure 1(b): \[\begin{equation}I = I_{0}\times A^{t}\end{equation}\]where t is number of days since \(I_{0}\) cases. This model only fits the blue line, and does not fit the trend of the yellow or green line at later stage. After doing some research on spread of infectious diseases, it seems an SIR-model might be a better model. The model consists of three parts: infected population, susceptible population and recovered/immune population. As I am trying to model the total of confirmed cases, not the active cases, for simplicity I will I can drop the recovery aspect of the model. This greatly simplifies the math, allowing to simply solve an ordinary differential equation. To further simplify this, let’s define population as 1 and I(t) is the percentage of population that is infected and S(t) is the proportion of susceptible population. Then we get: \[\begin{equation} S(t)+I(t) = 1\end{equation}\]Define \(\beta\) as transmission rate. Upon further reflection, the rate of infection should be affected by both infection and susceptible population.  To consider two extreme cases,  when s = 1 , i = 0, the transmission rate, β = 0 when s = 0 , i = 1, everyone is infected and no more new cases, β=0If we divide both side by \(\delta t\), we can get: \[\frac{dI}{dt} = - \frac{dS}{dt} = \beta S I\]This is equation is an ODE function, and we can solve it using scipy package. In fact, turns out that this function \(\begin{equation}\frac{dI}{dt} = \beta I \cdot (1-I)\end{equation}\) is the derivate of sigmoid function \[\begin{equation}I = \frac{1}{1+e^{-t}}\end{equation}\]Prediction using sigmoid function: The model predicts the total confirmed cases next 30 days. For example, given data on March 1st, the model estimates about 172172 confirmed cases in Italy at the end of April. Thanks for reading ! "
    }, {
    "id": 14,
    "url": "http://localhost:4000/anime_recommender/",
    "title": "Anime Recommender",
    "body": "2019/05/28 - In recent years, anime has become increasingly popular worldwide, and the number of anime available online has grown exponentially. With so many options to choose from, it can be challenging to select the perfect one to suit their unique tastes. To address this issue, machine learning techniques has been used to develop personalized anime recommendation systems. By analyzing an individual’s viewing history and preferences, these systems can suggest anime titles that are more likely to appeal to them, based on factors such as genre, themes, and style. In this way, anime fans can discover new shows that are tailored to their specific interests, and spend less time searching. This article explores the use of machine learning in anime recommendation systems and discusses how they can enhance the anime viewing experience. The process began by gathering data from MyAnimeList, a website dedicated to anime similar to IMDb. Over 5000 anime titles and user profiles were collected using Scrapy, Beautiful-soup and stored as JSON objects in MongoDB. The collected information includes names, descriptions, directors, vocal casts, theme songs, reviews, and more. Below is an example demonstrating the format of the collected data. Examples of data stored in MangoDB. {'_id': 'Yaiba', 'themesongs': [[' Yuuki ga Areba (勇気があれば)  by Kabuki Rocks (カブキロックス)',  ' Shinchigakunaki Tatakai! (神智学無き戦い!)  by Kabuki Rocks (カブキロックス)']], 'description': [ Kurogane Yaiba is a boy who doesn't want to become what any regular kid would: A samurai. That's why he undergoes a hard training with his father, knowing only the forest as his world. Then, one day, he is sent to Japan, where he has to deal with a whole new civilized reality, meeting the Mine family, the evil Onimaru and even the legendary Musashi, having lots of dangerous adventures, becoming stronger everyday. (Source: ANN, edited)  ], 'reviews': ['which are really stupid but it all succeds in tickling us!!the storycharacter and enjoyment is quite okwell i personally disliked the op and ed and art also seems quite ok {not many cute girls :( }its a lot of fun overall the series i ll definately say give 1 shot only to the 1st epi!!!ull automatically get hooked to the series atleast i did !well i hope u liked my review plz ratemy 1st reviewread more'], 'img_url': 'https://myanimelist. cdn-dena. com/images/anime/5/71953. jpg'}The table illustrates an example of a user’s watched history, and the accompanying table displays ratings for 5 different anime given by 5 distinct users. The ratings are on a scale of 1 to 10, with 10 representing the highest level of favorability.       Anime/score   User1   User2   User3   User4   User5         Fairy Tail   10   8   10   5           Kimi no na wa   10   9           4       No game no life   9       9               Tokyo Ghoul   7       9               One Piece               10   9   How does this recommender system works ?: Broadly speaking, there are two common algorithms used in the recommendation systems, collaborative filtering and content-based filtering. Collaborative filtering works by analyzing the viewing histories and ratings of multiple users. The system identifies users who have similar viewing and rating patterns and groups them into clusters. It then makes recommendations based on the preferences of users in the same cluster. For example, if many Marvel fans has enjoyed Tom and Jerry in their past, the system will likely recommend Tom and Jerry to those Marvel fans who has not watched it yet. In other words, it makes predictions based on the response of other users who share similar tastes. Content-based filtering, on the other hand, makes recommendations based on the content of the anime themselves. This approach involves analyzing the attributes, such as genre, theme, plot, character and story background,. The system then recommends anime to users based on their preferences for these attributes. For example, if a user enjoys watching romantic comedies with high school settings, the system will search for anime with similar attributes. Both methods have their pros and cons. A major appeal of collaborative filtering is its flexibility in dealing with various data aspects. Collaborative filtering requires an active user data base with effective rating system and it does not work well with new user profiles or new anime with no ratings or reviews, known as cold start problem. It also relies heavily on the availability of user data, which can lead to sparsity and bias in data. A content based filtering is more friendly to new anime but is more exclusive to users’ own experience, and does not consider social factors such as popularity. A more effective solution would be a hybrid system that combines both methods. While there are many blogs online discussing these two methods, few dive into how they work in practice. To begin with, it is important to understand the concept of the cold start problem and how it arises. We can conceptualize the rating system as a matrix, denoted as Rrating, where the matrix contains the scores of all anime titles rated by all users. In our particular case, the matrix size will be 5000 shows by 5000 users, and each row will represent the ratings given by a user, with a value of 0 indicating that the user has not watched the anime. [[ 10 0  0  0  8  9  0  9  0  0  10 0  5  0  10  . . . ] [ 0  0  0  9  0  0  0  8  0  0  4  0  0  0  8  . . . ] [ 9  0  0  0  0  8  9  0  0  0  0  0  0  0  0  . . . ] [ 0  3  1  0  0  0  0  0  7  3  9  0  0  0  0  . . . ] [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  . . . ]]Let’s take a look at Row 5, which represents a new user. Typically, most users will have only watched a small fraction of the thousands of anime titles available on the platform. Assuming an average user spends about one hour per week on anime and each show has around 12 episodes of 20 minutes each, that would amount to roughly 152 hours per year, or about 20 shows per year. Consequently, the rating matrix will be extremely sparse, with most elements being zero. As the platform expands and more users join, the sparsity problem will continue to grow more severe. Figure 1. Hybrid recommender work flow. A layout structure of my code. A: Collaborative Filtering WorkflowThe following image displays a user-item interaction matrix obtained from the ratings of six shows given by six users. Traditional collaborative filtering include measuring user similarity by calculating Pearson correlation or cosine similarity between normalized user vectors. Then combine the weighted average scores given by neighbors to estimate user’s score on the unseen show. A modern solution called matrix factorization, initially introduced by Simon Funk in 2006 in the Netflix Prize competition, has a better approach in handling this user-item matrix.  Instead of directly computing similarity between users, matrix factorization transforms the original matrix into two lower-dimensional matrices - one representing users and the other representing items using technique called singular value decomposition (SVD). These lower-dimensional matrices capture the latent factors or features that determine the user’s preference for a particular item, and can be used to predict missing ratings. Matrix factorization can provide more accurate predictions and is more scalable than traditional collaborative filtering methods. Figure 2. User-item matrix decomposition \(R=\begin{bmatrix}5 &amp; 3 &amp; 0 &amp; 1 \\4 &amp; 0 &amp; 0&amp; 1\\1 &amp; 1 &amp; 0&amp; 5\\0 &amp; 1 &amp; 5&amp; 4\\\end{bmatrix}\rightarrow\;\;\;\; U \times V\;\;\rightarrow \;\;\hat{R}=\begin{bmatrix}4. 9&amp; 2. 5 &amp;2. 2 &amp;0. 9 \\4. 2 &amp; 0. 5 &amp; 3. 4&amp; 1\\1. 5 &amp; 0. 3 &amp; 4. 5&amp; 3. 9\\1. 1 &amp; 0. 9 &amp; 4. 9&amp; 3. 2\\\end{bmatrix}\)Rating matrix R can be expressed as the product of two lower-dimensional matrices: U (user matrix) and V (item matrix). Assume k=3 latent factors, which means we assume that there are three underlying factors that determine how users rate the items (e. g. action vs romance vs adventure, animation quality vs length vs vocal, etc). We initialize U and V randomly, then use gradient descent to optimize the matrices based on the mean squared error loss between the predicted ratings and actual ratings in R. We can then use these matrices to predict the missing values in R, by taking the dot product of the corresponding user and item vectors. B: Content-based Filtering workflowA way to understand content-based filtering is to see it as a classification problem, where the system identifies relevant features in the content that are highly correlated with the user’s preferences. Recommendations are then made by comparing the user’s profile to the content of each item in the collection. The inputs are descriptions of anime stories and the goal is to identify the key topics or themes present in the text by extracting and grouping related keywords. TF-IDF is a common method used for text extraction that calculates the importance of a specific word in a given document. Words that appear frequently across all documents, such as “people” or “place,” are given a lower importance than words that appear infrequently but suggest a specific theme or topic, such as “wolf,” “magic,” or “spirit”.  In this project, 30 different anime topics were identified by extracting the key phrases from all the story descriptions. For instance, Topic 7 is centered on solving crimes, with Detective Conan being the most representative example. Examples:  Topic #1: Special, release, air, recap, featureTopic #2: Earth, Planet, space, alien, shipTopic #3: High, school, junior, student, classmateTopic #4: Team, soccer, player, match, baseballTopic #5: Human, race, mankind, god, survive, extinct Topic #6: Magic, witch, magician, kingdom, wishTopic #7: Mystery, solve, appear, past, shadow, killTopic #8: Demon, king, hero, lord, seal, defeat, missionTopic #9: Love, feel, fall, relationship, confess, heart Then we can calculate an associated probability for each topic for a given anime.  For example       Anime/Topics   Topic 2   Topic 3   Topic 7   Topic 8   Topic 9         Fairy Tail   0. 01   0. 12   0. 00   0. 67   0. 00       Kimi no na wa   0. 11   0. 87   0. 02   0. 01   0. 54       No game no life   0. 35   0. 23   0. 34   0. 01   0. 35       Tokyo Ghoul   0. 02   0. 56   0. 76   0. 12   0. 00       One Piece   0. 05   0. 00   0. 00   0. 45   0. 00   We can determine a user’s preference for different topics by taking an average of all the anime topics they have watched in the past. To generate recommendations, we then calculate the cosine similarity between the user’s topic matrix and the anime topic matrix. Finally, we combine the results obtained from the two recommenders. Future Improvements: Incorporating contextual information, such as location, country, ethnicity, age to enhance the accuracy of recommendations by tailoring them to the user’s current situation. Use deep learning techniques, such as neural networks, to create more sophisticated models that can capture more complex patterns and relationships between users and items. Conclusion: Both methods have their strengths and limitations, and they can be combined to create a hybrid recommender system. The effectiveness of these methods can be further improved with advancements in natural language processing and machine learning techniques. Overall, recommender systems play a critical role in providing personalized experiences to users and are increasingly important in today’s digital landscape. Reference "
    }, {
    "id": 15,
    "url": "http://localhost:4000/illegal_mining/",
    "title": "Amazon Illegal Mining Detection",
    "body": "2019/01/18 - The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive practice has led to significant habitat destruction, biodiversity loss, and contributes to climate change. A key factor exacerbating this devastation is the sharp increase in illegal gold mining activities observed in the region over the past decade. Motivated by the soaring global gold price, individuals have engaged in unlawful mining practices within the Amazon rainforest, further exacerbating the environmental crisis. Traditional labor-intensive methods involving the hiring of local citizens to spot illegal activities have proven to be costly and time-consuming. Fortunately, advancements in satellite technology have opened new avenues for detecting illegal mining activities using satellite images and employing machine learning techniques like image classification. The primary objective of this project is to develop an algorithm for labeling unlabeled image chips. The primary focus is on detecting the occurrence of ‘artisanal mining,’ commonly referred to as illegal mining, among a set of 16 labels categorized into three main groups:  atmospheric conditions, common land uses and rare land uses.  Image chips may receive none or more than one label from these groups. The main objectives include:    Train a Model with Labeled Image Chips:      Develop an algorithm that effectively labels unlabeled satellite image chips, considering atmospheric conditions and land uses as label categories.       Detection of Artisanal Mining (Illegal Mining):      Create a model that reliably identifies the presence of ‘artisanal mining’ (illegal mining) as one of the rare labels among others.    Differentiate between various classes of land cover and land use, emphasizing the detection of illegal mining activities.       Improved Understanding of Environmental Conditions:          Utilize the model to enhance the understanding of environmental conditions, particularly in relation to deforestation and illegal mining activities on a global scale.           Contribute to the identification and interpretation of patterns and causes of deforestation through the labeled satellite imagery.       ChallengesThe challenge of the project lies in addressing the highly imbalanced distribution of data labels among the 16 groups. Specifically, the difficulty arises from the fact that the ‘artisanal mine’ label constitutes less than 1% of the total population.     Left Figure 1: Examples of image chips with labels.  Right Figure 2: Frequency distribution of labels Key Challenges:  Blurry Image Challenge:     A significant amount of satellite image chips appears to be blurry. Therefore a dehazing function may be necessary to enhance the clarity of these images before feeding them into the model.    Addressing blurry images is crucial for ensuring that the model can effectively learn and make accurate predictions, particularly in regions where haziness may impact the interpretability of features.     Imbalanced Data Distribution:     Imbalanced data can lead to model bias, where the model may struggle to accurately identify and classify the minority class due to insufficient examples for learning.     Risk of Misclassification:     The imbalanced distribution increases the risk of misclassification, especially for the minority class. The model may be heavily biased towards predicting the majority classes.     Model Training Complexity:     The model needs to be trained to handle the rarity of the ‘artisanal mine’ label while maintaining performance across the other more prevalent labels.     Data Augmentation and Sampling Strategies:     Balancing techniques, such as oversampling, undersampling, or the use of synthetic data, may need to be implemented to ensure fair learning across all classes.    First AttemptMy initial attempt involved using a random forest with computed features such as haziness, image contrast, and RGB color channels. However, this approach proved unsuccessful, as the model struggled to differentiate the most basic weather conditions. Consequently, attention shifted towards a convolutional neural network (CNN), known for its prowess in capturing and extracting essential local features for pattern and object identification. While successful in discerning weather labels and common land uses, the CNN faced challenges in predicting rare land use labels, despite extensive training. Even after implementing oversampling techniques, the model not only failed to improve accuracy but also exacerbated the recall score, resulting in misclassifications of other labels as mining activities. In light of those challenges, I explored alternative approaches to improve model performance, particularly in predicting the 1% population representing rare land use categories.  Update computational power: use AWS GPU More advanced network designs Improve input image quality Rotate and flip images to increase the sample sizeUpdate computational powerTo harness the power of advanced hardware and accelerate the training process, an AWS GPU instance was utilized. The use of GPU computing combined with multi-processing technique allows for parallel processing and faster model training, enabling more efficient experimentation and optimization.  Additionally, cross-validation was implemented to continuously monitor the model’s performance across multiple iterations, ensuring that overfitting issues were identified and addressed promptly. Advanced network designs - DenseNetTo further improve the model’s performance and accuracy, a customized neural net model was implemented using DenseNet architecture. This deep neural network design introduces dense connections between layers, allowing for direct information flow and gradient propagation throughout the network. This architecture alleviates the vanishing gradient problem, encourages feature reuse and leads to better parameter efficiency and faster convergence during training. The Python package named Keras was employed to create and connect different neural network layers, with binary cross-entropy serving as the loss function. [\text{Binary Crossentropy } = -\sum y_i log({\hat{y}}_i)+(1-y_i)log({1-\hat{y}}_i)] , where y is the true probability and ŷ is the predicted probability. The binary cross-entropy loss function is particularly effective in multi-class problems. In this context, let’s consider a three-class problem with  \(y_A=1, y_B=1, y_C=0\).  Suppose the model predicts \(ŷ_A=0. 4, ŷ_B=0. 7, ŷ_C=0. 1\).  The loss is calculated as -(log(0. 4)+log(0. 7)+log(0. 9)) = 0. 60. As the predicted y of A and B approaches the correct answer 1, the loss function becomes small. Conversely, if predicted probability of A is close to zero, the loss becomes a large number. Same applies to \(y_C\) when it approaches to 0. As a result, the new model benefits from requiring fewer parameters and faster convergence compared to CNN. It accelerates the training process and mitigates overfitting risk. This approach encourages the model to learn more robust and generalized features from the data, ultimately improving performance and accuracy. Improve image qualitySatellite images often suffer from darkness and blurriness caused by atmospheric turbulence. However, by mitigating the effects of haze and improving image quality, we can enhance the performance of neural networks. Leveraging my understanding of atmospheric physics and conducting research on Google Scholars, I developed a dehaze function based on a paper to address this challenge. Haze results from the scattering of light in the atmosphere before it reaches the camera. To estimate the intensity of scattered light, a constant value is derived by approximating the maximum pixel intensity within the darkest RGB channel. By utilizing OpenCV to convert images to a colorspace matrix and calculating the haze constant, we can restore the image by subtracting this value. As a result, the dehazed image appears brighter and exhibits better contrast, improving the quality of input data for neural networks. Figure 3: Formation of a hazy image. Camera = Direct attenuation+Airlight Figure4 : Examples of satellite image chips before and after haze removal The impact of the dehazing process is evident, as it leads to a substantial improvement in image quality across most samples. Upon the application of the haze removal function, the image undergoes a remarkable transformation. The contrast is restored, bringing out more vivid and distinct features. The sharpness of objects is improved, and details that were previously obscured by haze become more visible. Colors, regaining vibrancy and accuracy, contribute to a clearer and more faithful representation of the scene. Overall, the haze removal function significantly elevates the image visibility and quality by reducing the impact of atmospheric interference. The choice of F2 score as the performance metric is deliberate due to the uneven distribution of label classes, where recall takes precedence over precision. Recall measures the ability to capture all positive instances correctly. In the specific context of identifying labels associated with mining, a high recall implies the model’s effectiveness in detecting actual mining instances, thereby minimizing the occurrence of false negatives (missed mining instances). The F2 score, by assigning greater weight to recall, ensures that the model adeptly identifies as many positive instances as possible while maintaining a reasonable level of precision. This strategic emphasis aligns with the objective of optimizing performance in scenarios where missing positive instances carries a higher cost than false positives.  Figure 5: F2 score before (purple) and after (blue) the haze removal function. Notice it dramatically improve the results for rare land use labels Work Flow Model performanceAfter training the DenseNet model on the dehazed image sets, the model was saved as “b01_dense121. h5”. This trained model was then utilized to generate labels for the previously unlabelled test set images. The model successfully identified several test photo chips as illegal mines, providing valuable insights into the presence of illegal mining activities in the dataset. Figure 6. Examples from the test data that were marked as illegal mines with labels generated by the DenseNet model. In summary, the project focused on developing a model to detect illegal mining and rare land use cases using satellite images. Several approaches were employed to address the challenges of imbalanced labels, low image quality, and underfitting. These approaches included improving computational power, rotating images of rare land use categories, using more advanced network designs like DenseNet, and enhancing input image quality through haze removal. The implementation of these approaches resulted in significant improvements in both precision and recall, particularly for the rare land use cases. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        
<div class="container">
<div class="jumbotron jumbotron-fluid mb-3 pl-0 pt-0 pb-0 bg-white position-relative">
		<div class="h-100 tofront">
			<div class="row  justify-content-between ">
				<div class=" col-md-6  pr-0 pr-md-4 pt-4 pb-4 align-self-center">
					<p class="text-uppercase font-weight-bold">
                        <span class="catlist">
						
                          <a class="sscroll text-danger" href="/categories.html#classification">classification</a><span class="sep">, </span>
                        
                          <a class="sscroll text-danger" href="/categories.html#supervised">supervised</a><span class="sep">, </span>
                        
                        </span>
					</p>
					<h1 class="display-4 mb-4 article-headline">Amazon Illegal Mining Detection</h1>
					<div class="d-flex align-items-center">
                        
                        <img class="rounded-circle" src="/assets/images/avatar.jpeg" alt="Penny" width="70"/>
                        
						<small class="ml-3"> Penny
                            <span class="text-muted d-block mt-1">Jan 18, 2019 · <span class="reading-time">
  
  
    9 mins read
  
</span>
    </span>
						</small>
					</div>
				</div>
                
				<div class="col-md-6 pr-0 align-self-center">
					<img class="rounded" src="/assets/images/1-mining.jpeg" alt="Amazon Illegal Mining Detection">
				</div>
                
			</div>
		</div>
	</div>
</div>




<div class="container-lg pt-4 pb-4">
	<div class="row justify-content-center">


        <!-- Share -->
		<div class="col-lg-1">

		</div>


		<div class="col-md-12 col-lg-8">

            <!-- Article -->
			<article class="article-post">
			<p>The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive practice has led to significant habitat destruction, biodiversity loss, and contributes to climate change. A key factor exacerbating this devastation is the sharp increase in illegal gold mining activities observed in the region over the past decade. Motivated by the soaring global gold price, individuals have engaged in unlawful mining practices within the Amazon rainforest, further exacerbating the environmental crisis.</p>

<p>Traditional labor-intensive methods involving the hiring of local citizens to spot illegal activities have proven to be costly and time-consuming. Fortunately, advancements in satellite technology have opened new avenues for detecting illegal mining activities using satellite images and employing machine learning techniques like image classification.</p>

<p>The primary objective of this project is to develop an algorithm for labeling unlabeled image chips. The primary focus is on detecting the occurrence of ‘artisanal mining,’ commonly referred to as illegal mining, among a set of 16 labels categorized into three main groups: <span style="color:SALMON">  atmospheric conditions</span>,  <span style="color:LightSkyBlue">common land uses</span> and <span style="color:Plum"> rare land uses</span>.  Image chips may receive none or more than one label from these groups.</p>

<p>The main objectives include:</p>

<ol>
  <li>
    <p><strong>Train a Model with Labeled Image Chips:</strong></p>

    <ul>
      <li>Develop an algorithm that effectively labels unlabeled satellite image chips, considering atmospheric conditions and land uses as label categories.</li>
    </ul>
  </li>
  <li>
    <p><strong>Detection of Artisanal Mining (Illegal Mining):</strong></p>

    <ul>
      <li>Create a model that reliably identifies the presence of ‘artisanal mining’ (illegal mining) as one of the rare labels among others.</li>
      <li>Differentiate between various classes of land cover and land use, emphasizing the detection of illegal mining activities.</li>
    </ul>
  </li>
  <li>
    <p><strong>Improved Understanding of Environmental Conditions:</strong></p>

    <ul>
      <li>
        <p>Utilize the model to enhance the understanding of environmental conditions, particularly in relation to deforestation and illegal mining activities on a global scale.</p>
      </li>
      <li>
        <p>Contribute to the identification and interpretation of patterns and causes of deforestation through the labeled satellite imagery.</p>
      </li>
    </ul>
  </li>
</ol>

<h5 id="challenges">Challenges</h5>

<p>The challenge of the project lies in addressing the highly imbalanced distribution of data labels among the 16 groups. Specifically, the difficulty arises from the fact that the ‘artisanal mine’ label constitutes less than 1% of the total population.</p>

<p float="left">
    <img src="https://live.staticflickr.com/65535/49626992868_557450fa33.jpg" width="50%" />  <img src="https://lh3.googleusercontent.com/jn0yWdVFz-RplTsir-DZcRs0UYWSouwjwhknKi3J6-f-o4TPWBlL2AGNsKQa0NIBkPJ66XfUfKrB03-BmHo8vDq2dJhf6lZLRuhQmluBukP2V979NtW7NZ-5odX8mhEru029s6PDy40" width="49%" /> <em>Left Figure 1: Examples of image chips with labels.   Right Figure 2: Frequency distribution of labels</em></p>

<p>Key Challenges:</p>

<ol>
  <li><strong>Blurry Image Challenge:</strong>
    <ul>
      <li>A significant amount of satellite image chips appears to be blurry. Therefore a dehazing function may be necessary to enhance the clarity of these images before feeding them into the model.</li>
      <li>Addressing blurry images is crucial for ensuring that the model can effectively learn and make accurate predictions, particularly in regions where haziness may impact the interpretability of features.</li>
    </ul>
  </li>
  <li><strong>Imbalanced Data Distribution:</strong>
    <ul>
      <li>Imbalanced data can lead to model bias, where the model may struggle to accurately identify and classify the minority class due to insufficient examples for learning.</li>
    </ul>
  </li>
  <li><strong>Risk of Misclassification:</strong>
    <ul>
      <li>The imbalanced distribution increases the risk of misclassification, especially for the minority class. The model may be heavily biased towards predicting the majority classes.</li>
    </ul>
  </li>
  <li><strong>Model Training Complexity:</strong>
    <ul>
      <li>The model needs to be trained to handle the rarity of the ‘artisanal mine’ label while maintaining performance across the other more prevalent labels.</li>
    </ul>
  </li>
  <li><strong>Data Augmentation and Sampling Strategies:</strong>
    <ul>
      <li>Balancing techniques, such as oversampling, undersampling, or the use of synthetic data, may need to be implemented to ensure fair learning across all classes.</li>
    </ul>
  </li>
</ol>

<h5 id="first-attempt">First Attempt</h5>

<p>My initial attempt involved using a random forest with computed features such as haziness, image contrast, and RGB color channels. However, this approach proved unsuccessful, as the model struggled to differentiate the most basic weather conditions. Consequently, attention shifted towards a convolutional neural network (CNN), known for its prowess in capturing and extracting essential local features for pattern and object identification. While successful in discerning weather labels and common land uses, the CNN faced challenges in predicting rare land use labels, despite extensive training. Even after implementing oversampling techniques, the model not only failed to improve accuracy but also exacerbated the recall score, resulting in misclassifications of other labels as mining activities.</p>

<p>In light of those challenges, I explored alternative approaches to improve model performance, particularly in predicting the 1% population representing rare land use categories.</p>

<ul>
  <li>Update computational power: use AWS GPU</li>
  <li>More advanced network designs</li>
  <li>Improve input image quality</li>
  <li>Rotate and flip images to increase the sample size</li>
</ul>

<h5 id="update-computational-power">Update computational power</h5>

<p>To harness the power of advanced hardware and accelerate the training process, an AWS GPU instance was utilized. The use of GPU computing combined with multi-processing technique allows for parallel processing and faster model training, enabling more efficient experimentation and optimization.   Additionally, cross-validation was implemented to continuously monitor the model’s performance across multiple iterations, ensuring that overfitting issues were identified and addressed promptly.</p>

<h5 id="advanced-network-designs---densenet">Advanced network designs - DenseNet</h5>

<p>To further improve the model’s performance and accuracy, a customized neural net model was implemented using DenseNet architecture. This deep neural network design introduces dense connections between layers, allowing for direct information flow and gradient propagation throughout the network. This architecture alleviates the vanishing gradient problem, encourages feature reuse and leads to better parameter efficiency and faster convergence during training.The Python package named Keras was employed to create and connect different neural network layers, with binary cross-entropy serving as the loss function.</p>

\[\text{Binary Crossentropy } = -\sum y_i log({\hat{y}}_i)+(1-y_i)log({1-\hat{y}}_i)\]

<p>, where y is the true probability and ŷ  is the predicted probability. The binary cross-entropy loss function is particularly effective in multi-class problems. In this context, let’s consider a three-class problem with   \(y_A=1, y_B=1, y_C=0\).  Suppose the model predicts \(ŷ_A=0.4, ŷ_B=0.7, ŷ_C=0.1\).  The loss is calculated as -(log(0.4)+log(0.7)+log(0.9)) = 0.60. As the predicted y of A and B approaches the correct answer 1, the loss function becomes small. Conversely, if predicted probability of A is close to zero, the loss becomes a large number. Same applies to \(y_C\) when it approaches to 0.</p>

<p>As a result, the new model benefits from requiring fewer parameters and faster convergence compared to CNN. It accelerates the training process and mitigates overfitting risk. This approach encourages the model to learn more robust and generalized features from the data, ultimately improving performance and accuracy.</p>

<h5 id="improve-image-quality">Improve image quality</h5>

<p>Satellite images often suffer from darkness and blurriness caused by atmospheric turbulence. However, by mitigating the effects of haze and improving image quality, we can enhance the performance of neural networks. Leveraging my understanding of atmospheric physics and conducting research on Google Scholars, I developed a dehaze function based on a <a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/hazeremoval.pdf">paper</a> to address this challenge. Haze results from the scattering of light in the atmosphere before it reaches the camera. To estimate the intensity of scattered light, a constant value is derived by approximating the maximum pixel intensity within the darkest RGB channel. By utilizing OpenCV to convert images to a colorspace matrix and calculating the haze constant, we can restore the image by subtracting this value. As a result, the dehazed image appears brighter and exhibits better contrast, improving the quality of input data for neural networks.</p>

<p><img src="https://www.researchgate.net/profile/Seung_Won_Jung2/publication/291385074/figure/fig14/AS:320880610693124@1453515307125/Formation-of-a-hazy-image.png" width="80%" />
<em>Figure 3: Formation of a hazy image. Camera = Direct attenuation+Airlight</em></p>

<p><img src="https://live.staticflickr.com/65535/53340498376_0a75c43194_c.jpg" width="80%" />
<em>Figure4 : Examples of satellite image chips before and after haze removal</em></p>

<p>The impact of the dehazing process is evident, as it leads to a substantial improvement in image quality across most samples. Upon the application of the haze removal function, the image undergoes a remarkable transformation. The contrast is restored, bringing out more vivid and distinct features. The sharpness of objects is improved, and details that were previously obscured by haze become more visible. Colors, regaining vibrancy and accuracy, contribute to a clearer and more faithful representation of the scene. Overall, the haze removal function significantly elevates the image visibility and quality by reducing the impact of atmospheric interference.</p>

<p>The choice of F2 score as the performance metric is deliberate due to the uneven distribution of label classes, where recall takes precedence over precision. Recall measures the ability to capture all positive instances correctly. In the specific context of identifying labels associated with mining, a high recall implies the model’s effectiveness in detecting actual mining instances, thereby minimizing the occurrence of false negatives (missed mining instances). The F2 score, by assigning greater weight to recall, ensures that the model adeptly identifies as many positive instances as possible while maintaining a reasonable level of precision. This strategic emphasis aligns with the objective of optimizing performance in scenarios where missing positive instances carries a higher cost than false positives.</p>

<p><img style="width:80%;display:inline-block;" src="https://lh6.googleusercontent.com/P4qLeEVjt-Xh1vPbRrR12i6W43sfm03gZnA5x4NAoSkD4rkqx5cYPlmu9EplmXZDWM0TDudJzw-OOGQIOJ26T4VAFf2sD6isNkzWEyyZJOosXJpH5xXg581AVMpYm1B8j007y6BbdXk" /></p>

<p><em>Figure 5: F2 score before (purple) and after (blue) the haze removal function. Notice it dramatically improve the results for rare land use labels</em></p>

<h5 id="work-flow">Work Flow</h5>

<p><img src="https://live.staticflickr.com/65535/49627827697_8058d80cab_c.jpg" width="80%" alt="work flow" /></p>

<h5 id="model-performance">Model performance</h5>

<p>After training the DenseNet model on the dehazed image sets, the model was saved as “b01_dense121.h5”. This trained model was then utilized to generate labels for the previously unlabelled test set images. The model successfully identified several test photo chips as illegal mines, providing valuable insights into the presence of illegal mining activities in the dataset.</p>

<p><img src="https://live.staticflickr.com/65535/49627526151_a93a067f6d_c.jpg" width="100%" /><em>Figure 6. Examples from the test data that were marked as illegal mines with labels generated by the DenseNet model.</em></p>

<p>In summary, the project focused on developing a model to detect illegal mining and rare land use cases using satellite images. Several approaches were employed to address the challenges of imbalanced labels, low image quality, and underfitting. These approaches included improving computational power, rotating images of rare land use categories, using more advanced network designs like DenseNet, and enhancing input image quality through haze removal. The implementation of these approaches resulted in significant improvements in both precision and recall, particularly for the rare land use cases.</p>


			</article>

			<!-- Tags -->
			<div class="mb-4">
				<span class="taglist">
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#sticky">sticky</a>
				
				</span>
			</div>


             <!-- Author Box -->
                
				<div class="row mt-5">
					<div class="col-md-2 align-self-center">
                         
                        <img class="rounded-circle" src="/assets/images/avatar.jpeg" alt="Penny" width="90"/>
                        
					</div>
					<div class="col-md-10">
                        <h5 class="font-weight-bold">Written by Penny</h5>
						Data scienctist, love to explore new ideas
					</div>
				</div>
                


		</div>


	</div>
</div>


<!-- Aletbar Prev/Next -->
<div class="alertbar">
    <div class="container">
        <div class="row prevnextlinks small font-weight-bold">
          
          
            <div class="col-md-6 text-right pr-0">
                <a class="text-dark" href="/anime_recommender/"> Anime Recommender </a>
				<i class='fas fa-angle-right fa-lg' style='color:#6610f2'></i>

            </div>
          
        </div>
    </div>
</div>

    </main>



    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Penny</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/mumuxi15/02-Data-Science-Project"><i class="fab fa-github"></i> Github Projects</a>

            </div>
            <div>
                Made with Jekyll Theme
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
