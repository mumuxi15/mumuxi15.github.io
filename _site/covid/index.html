<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Coronavirus Data Analysis | Penny Pan</title>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Coronavirus Data Analysis | Penny Pan</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Coronavirus Data Analysis" />
<meta name="author" content="penny" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Subtitle: COVID19 Spread SIR Model Simulation using Python" />
<meta property="og:description" content="Subtitle: COVID19 Spread SIR Model Simulation using Python" />
<link rel="canonical" href="http://localhost:4000/covid/" />
<meta property="og:url" content="http://localhost:4000/covid/" />
<meta property="og:site_name" content="Penny Pan" />
<meta property="og:image" content="https://safeharborsc.imgix.net/adobestock-339124602-converted-1731501992.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-14T00:00:00-04:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://safeharborsc.imgix.net/adobestock-339124602-converted-1731501992.png" />
<meta property="twitter:title" content="Coronavirus Data Analysis" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"penny"},"dateModified":"2020-03-14T00:00:00-04:00","datePublished":"2020-03-14T00:00:00-04:00","description":"Subtitle: COVID19 Spread SIR Model Simulation using Python","headline":"Coronavirus Data Analysis","image":"https://safeharborsc.imgix.net/adobestock-339124602-converted-1731501992.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/covid/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"penny"},"url":"http://localhost:4000/covid/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Penny Pan</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": ""
    }, {
    "id": 2,
    "url": "http://localhost:4000/author-penny.html",
    "title": "Penny",
    "body": "                    {{page. title}}:         {{ site. authors. penny. site }}         {{ site. authors. penny. bio }}                                   Posts by {{page. title}}:       {% assign posts = site. posts | where: author , penny  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 3,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "{{page. title}}:     {% for author in site. authors %}                                         {{ author[1]. name }} :       (View Posts)      {{ author[1]. bio }}                                           {% endfor %}  "
    }, {
    "id": 4,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 5,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 6,
    "url": "http://localhost:4000/",
    "title": "Free Jekyll Theme",
    "body": "{% if page. url ==  /  %}    {% assign latest_post = site. posts[0] %}        &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               {{ latest_post. date | date: '%b %d, %Y' }}                      {%- assign second_post = site. posts[1] -%}            {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                            {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                            {{ second_post. date | date: '%b %d, %Y' }}                                {%- assign third_post = site. posts[2] -%}            {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                            {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                            {{ third_post. date | date: '%b %d, %Y' }}                                {%- assign fourth_post = site. posts[3] -%}            {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                            {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                            {{ fourth_post. date | date: '%b %d, %Y' }}                          {% for post in site. posts %}{% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More                        {% endif %}{% endfor %}{% endif %}       All Stories:     {% for post in paginator. posts %}      {% include main-loop-card. html %}    {% endfor %}               {% if paginator. total_pages &gt; 1 %}             {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 7,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 8,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags    {% for tag in site. tags %}    {{ tag[0] }}:     {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}      {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}              {% include sidebar-featured. html %}    "
    }, {
    "id": 9,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 10,
    "url": "http://localhost:4000/Optiver-stock-volatility-prediction/",
    "title": "Optiver stock volatility prediction",
    "body": "2022/05/12 - Project descriptionOptiver, a globally renowned high-frequency trading company, is organizing a Kaggle competition focused on discovering optimal volatility prediction models. The objective is to forecast short-term volatility for numerous stocks spanning various sectors. With access to millions of rows of detailed financial data, participants are challenged to anticipate volatility over 10-minute intervals. The performance of the models will be assessed using actual market data gathered during the three-month evaluation period following the training phase. First let’s have some basic understanding how is a trade is executed. When an investor wants to buy or sell, they submit a market order, which will match the best available price. Trades occur when a buyer’s bid matches a seller’s ask, leading to an agreement on the trade price.       Anime/score   User1   User2   User3   User4   User5         Fairy Tail   10   8   10   5           Kimi no na wa   10   9           4       No game no life   9       9               Tokyo Ghoul   7       9               One Piece               10   9   How does this recommender system works ?: Broadly speaking, there are two common algorithms used in the recommendation systems, collaborative filtering and content-based filtering. Collaborative filtering works by analyzing the viewing histories and ratings of multiple users. The system identifies users who have similar viewing and rating patterns and groups them into clusters. It then makes recommendations based on the preferences of users in the same cluster. For example, if many Marvel fans has enjoyed Tom and Jerry in their past, the system will likely recommend Tom and Jerry to those Marvel fans who has not watched it yet. In other words, it makes predictions based on the response of other users who share similar tastes. Content-based filtering, on the other hand, makes recommendations based on the content of the anime themselves. This approach involves analyzing the attributes, such as genre, theme, plot, character and story background,. The system then recommends anime to users based on their preferences for these attributes. For example, if a user enjoys watching romantic comedies with high school settings, the system will search for anime with similar attributes. Both methods have their pros and cons. A major appeal of collaborative filtering is its flexibility in dealing with various data aspects. Collaborative filtering requires an active user data base with effective rating system and it does not work well with new user profiles or new anime with no ratings or reviews, known as cold start problem. It also relies heavily on the availability of user data, which can lead to sparsity and bias in data. A content based filtering is more friendly to new anime but is more exclusive to users’ own experience, and does not consider social factors such as popularity. A more effective solution would be a hybrid system that combines both methods. While there are many blogs online discussing these two methods, few dive into how they work in practice. To begin with, it is important to understand the concept of the cold start problem and how it arises. We can conceptualize the rating system as a matrix, denoted as Rrating, where the matrix contains the scores of all anime titles rated by all users. In our particular case, the matrix size will be 5000 shows by 5000 users, and each row will represent the ratings given by a user, with a value of 0 indicating that the user has not watched the anime. [[ 10 0  0  0  8  9  0  9  0  0  10 0  5  0  10  . . . ] [ 0  0  0  9  0  0  0  8  0  0  4  0  0  0  8  . . . ] [ 9  0  0  0  0  8  9  0  0  0  0  0  0  0  0  . . . ] [ 0  3  1  0  0  0  0  0  7  3  9  0  0  0  0  . . . ] [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  . . . ]]Let’s take a look at Row 5, which represents a new user. Typically, most users will have only watched a small fraction of the thousands of anime titles available on the platform. Assuming an average user spends about one hour per week on anime and each show has around 12 episodes of 20 minutes each, that would amount to roughly 152 hours per year, or about 20 shows per year. Consequently, the rating matrix will be extremely sparse, with most elements being zero. As the platform expands and more users join, the sparsity problem will continue to grow more severe. Figure 1. Hybrid recommender work flow. A layout structure of my code. A: Collaborative Filtering WorkflowThe following image displays a user-item interaction matrix obtained from the ratings of six shows given by six users. Traditional collaborative filtering include measuring user similarity by calculating Pearson correlation or cosine similarity between normalized user vectors. Then combine the weighted average scores given by neighbors to estimate user’s score on the unseen show. A modern solution called matrix factorization, initially introduced by Simon Funk in 2006 in the Netflix Prize competition, has a better approach in handling this user-item matrix.  Instead of directly computing similarity between users, matrix factorization transforms the original matrix into two lower-dimensional matrices - one representing users and the other representing items using technique called singular value decomposition (SVD). These lower-dimensional matrices capture the latent factors or features that determine the user’s preference for a particular item, and can be used to predict missing ratings. Matrix factorization can provide more accurate predictions and is more scalable than traditional collaborative filtering methods. Figure 2. User-item matrix decomposition \(R=\begin{bmatrix}5 &amp; 3 &amp; 0 &amp; 1 \\4 &amp; 0 &amp; 0&amp; 1\\1 &amp; 1 &amp; 0&amp; 5\\0 &amp; 1 &amp; 5&amp; 4\\\end{bmatrix}\rightarrow\;\;\;\; U \times V\;\;\rightarrow \;\;\hat{R}=\begin{bmatrix}4. 9&amp; 2. 5 &amp;2. 2 &amp;0. 9 \\4. 2 &amp; 0. 5 &amp; 3. 4&amp; 1\\1. 5 &amp; 0. 3 &amp; 4. 5&amp; 3. 9\\1. 1 &amp; 0. 9 &amp; 4. 9&amp; 3. 2\\\end{bmatrix}\)Rating matrix R can be expressed as the product of two lower-dimensional matrices: U (user matrix) and V (item matrix). Assume k=3 latent factors, which means we assume that there are three underlying factors that determine how users rate the items (e. g. action vs romance vs adventure, animation quality vs length vs vocal, etc). We initialize U and V randomly, then use gradient descent to optimize the matrices based on the mean squared error loss between the predicted ratings and actual ratings in R. We can then use these matrices to predict the missing values in R, by taking the dot product of the corresponding user and item vectors. B: Content-based Filtering workflowA way to understand content-based filtering is to see it as a classification problem, where the system identifies relevant features in the content that are highly correlated with the user’s preferences. Recommendations are then made by comparing the user’s profile to the content of each item in the collection. The inputs are descriptions of anime stories and the goal is to identify the key topics or themes present in the text by extracting and grouping related keywords. TF-IDF is a common method used for text extraction that calculates the importance of a specific word in a given document. Words that appear frequently across all documents, such as “people” or “place,” are given a lower importance than words that appear infrequently but suggest a specific theme or topic, such as “wolf,” “magic,” or “spirit”.  In this project, 30 different anime topics were identified by extracting the key phrases from all the story descriptions. For instance, Topic 7 is centered on solving crimes, with Detective Conan being the most representative example. Examples:  Topic #1: Special, release, air, recap, featureTopic #2: Earth, Planet, space, alien, shipTopic #3: High, school, junior, student, classmateTopic #4: Team, soccer, player, match, baseballTopic #5: Human, race, mankind, god, survive, extinct Topic #6: Magic, witch, magician, kingdom, wishTopic #7: Mystery, solve, appear, past, shadow, killTopic #8: Demon, king, hero, lord, seal, defeat, missionTopic #9: Love, feel, fall, relationship, confess, heart Then we can calculate an associated probability for each topic for a given anime.  For example       Anime/Topics   Topic 2   Topic 3   Topic 7   Topic 8   Topic 9         Fairy Tail   0. 01   0. 12   0. 00   0. 67   0. 00       Kimi no na wa   0. 11   0. 87   0. 02   0. 01   0. 54       No game no life   0. 35   0. 23   0. 34   0. 01   0. 35       Tokyo Ghoul   0. 02   0. 56   0. 76   0. 12   0. 00       One Piece   0. 05   0. 00   0. 00   0. 45   0. 00   We can determine a user’s preference for different topics by taking an average of all the anime topics they have watched in the past. To generate recommendations, we then calculate the cosine similarity between the user’s topic matrix and the anime topic matrix. Finally, we combine the results obtained from the two recommenders. Future Improvements: Incorporating contextual information, such as location, country, ethnicity, age to enhance the accuracy of recommendations by tailoring them to the user’s current situation. Use deep learning techniques, such as neural networks, to create more sophisticated models that can capture more complex patterns and relationships between users and items. Conclusion: Both methods have their strengths and limitations, and they can be combined to create a hybrid recommender system. The effectiveness of these methods can be further improved with advancements in natural language processing and machine learning techniques. Overall, recommender systems play a critical role in providing personalized experiences to users and are increasingly important in today’s digital landscape. Reference "
    }, {
    "id": 11,
    "url": "http://localhost:4000/Ecommerce/",
    "title": "Building a Dynamic E-commerce Website for Small Businesses",
    "body": "2021/06/06 - The ongoing COVID-19 pandemic has brought about a seismic shift in consumer behavior, driving an unprecedented surge in online shopping. With an increasing preference for online shopping over in-person experiences, small business owners find themselves presented with a unique opportunity to expand their customer base and boost revenue by establishing a robust e-commerce presence. In this blog post, we’ll explore the transformative potential of setting up a simple yet dynamic website with a database for small businesses. This strategic move not only enables businesses to adapt to the changing landscape but also empowers them to thrive in the digital world. 1. Plan and Design: Crafting a Roadmap to Success: Before diving into the development process, it’s crucial to meticulously plan and design your e-commerce platform. Small businesses need to outline their goals, target audience, and unique selling propositions. A well-thought-out plan ensures that the subsequent steps align with the business’s objectives, providing a roadmap for success. To be more specific, consider the features you want to incorporate, such as color themes, payment gateway integration, and more. Popular payment gateways like PayPal, Stripe, Braintree, and Square offer API access, allowing for seamless integration and backtesting. 2. Set up coding environment: Initiate your project by establishing a virtual environment using tools like virtualenv, helps to keep project dependencies organized and prevents conflicts between different projects. Additionally, it simplifies the process of transitioning projects to different laptops or computers and enables project reproduction across different devices. Once you have created a virtual environment, you can activate it by running the command. virtualenv -p python3 &lt;desired-path&gt;source &lt;venv&gt;/bin/activate3. Building the project: Building a shopping site involves a multifaceted approach that encompasses various aspects, from designing the front end for user interaction to implementing a robust backend and database system. The front end is what users see in their browsers, while the back end is responsible for fulfilling user requests on the server side. You can either design the layout and animations yourself using CSS and JavaScript or utilize tools like Bootstrap, which expedites the front end creation of a page. When it comes to the backend, Flask stands out as an excellent choice for beginners. Its user-friendly nature, coupled with examples readily available on platforms like GitHub, makes it an ideal starting point. The micro-framework of Flask offers flexibility, allowing businesses to tailor their web applications to meet specific project needs. You can have a website up and displaying “hello world!” within seconds by utilizing the following code. import osfrom flask import Flaskdef create_app(test_config=None):  # create and configure the app  app = Flask(__name__, instance_relative_config=True)  app. config. from_mapping(    SECRET_KEY='dev',    DATABASE=os. path. join(app. instance_path, 'flaskr. sqlite'),  )  if test_config is None:    # load the instance config, if it exists, when not testing    app. config. from_pyfile('config. py', silent=True)  else:    # load the test config if passed in    app. config. from_mapping(test_config)  # ensure the instance folder exists  try:    os. makedirs(app. instance_path)  except OSError:    pass  # a simple page that says hello  @app. route('/hello')  def hello():    return 'Hello, World!'  return appflask --app flaskr run --debugThe database is the backbone of a shopping site, storing valuable information such as product details, customer data, and transaction history. Ask yourself a few questions before start.  Product Attributes:     What attributes of the product do we need to showcase to customers?   Consider aspects like product images, descriptions, prices, and any other relevant details.     Customer Information:     What information needs to be collected from customers?   Think about user registration details, shipping addresses, and any additional data relevant to your business model.    Drawing a clear and concise database schema is a critical step in visualizing the data model and establishing relationships between different entities. A standard schema typically includes tables representing various aspects of your website. Consider the following example: In this example, the grey lines depict how tables are linked. Notice the “order_detail” table sharing a common key, “order_id,” with the “orders” table. This linkage allows for accurate tracking of inventory levels and updating product availability as items are sold. Taking the time to design a well-structured database schema sets the foundation for a robust and scalable shopping website. Below is a simple example of a Flask application with a SQL database. In this example, we’ll create a basic shopping sites with products and orders.  SQLite is used for simplicity, but you can replace it with a more robust database like PostgreSQL or MySQL based on your needs. from flask import Flask, render_template, request, redirect, url_forfrom flask_sqlalchemy import SQLAlchemyapp = Flask(__name__)app. config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///shop. db' # Use SQLite for simplicitydb = SQLAlchemy(app)# Define Product and Order modelsclass Product(db. Model):  id = db. Column(db. Integer, primary_key=True)  name = db. Column(db. String(255), nullable=False)  price = db. Column(db. Float, nullable=False)class Order(db. Model):  id = db. Column(db. Integer, primary_key=True)  product_id = db. Column(db. Integer, db. ForeignKey('product. id'), nullable=False)  quantity = db. Column(db. Integer, nullable=False)Reference: Flask-login document Flask-login github demo Organzing Flask Apps with Blueprints As the website expands, consider enhancing the user experience with additional features and functionalities. Integration of payment gateways, search functionality, product reviews, wish lists, and social media integration can significantly boost user engagement and drive sales. Flask’s modular design and scalability allow you to seamlessly incorporate these features. Online tutorials from reputable sources, such as PayPal’s official guide on integrating PayPal Checkout for online payments, and various programming tutorials on building a shopping cart using JavaScript, provide valuable resources for developers. Reference: Paypal integration JavaScript Shopping Cart Tutorial 4. Testing: Testing is a crucial phase in website development, involving various tests like unit tests, integration tests, and end-to-end tests to verify different components’ functionality. Frameworks like Pytest and unittest can be employed for testing purposes, and debugging is essential to ensure smooth website functionality. 5. Deploy: Once testing is complete, the next step is to deployment. Hosting services like Heroku, AWS, and Google Cloud Platform provide platforms for deploying websites. You will need to configure the hosting environment, set up the database and server, and deploy your code to the server. It’s also vital to consider factors like scalability, security, and availability during deployment. I used pythoneverywhere. com to host my website. It is affordable and Python friendly. With a paid account, you can host up to three web applications for $5 per month. Security is paramount when handling sensitive customer information. It is our responsibility to protect user information from potential hackers and other security threats. An essential key is to use encryption to protect the data when transmitted between client and server. First, you’ll need to obtain an SSL (Secure Sockets Layer) certificate, which verifies the identity of your website and encrypts all data transmitted between the server and the client. You can obtain a SSL certificate from a trusted certificate authority such as Let’s Encrypt (FREE !!!) Once you have the certificate, configure the web server to use HTTPS instead of HTTP. HTTPS is a secure version of the HTTP protocol that encrypts all data transmitted between the client and server. In addition to using encryption, you should also take other security measures such as regularly updating your software and plugins, using strong passwords, and implementing measures such as two-factor authentication to protect your website from potential threats. It’s also a good idea to regularly back up your website to protect against data loss in the event of a security breach or other issue. 6. Google Search: Ensuring visibility on Google search results is pivotal for the success of your website. Google Search Console provides tools for verification and indexing. Purchasing a domain name, completing DNS verification, and submitting the index request are steps to ensure visibility in Google search results.    Verify the domain on google search console. You can open the console by typing in google   site:xxx. com      Complete the DNS verification as google requested     Buy a domain name.     Edit CNAME record in the DNS manage section. Here I used goDaddy as an example.  host: wwwpoints to: yourwebsitename. com Then go back to google console, search for your url in the URL inspection section and submit the index request. Google will perform several test on the website and if passed, you will receive a confirmation email from google. In conclusion, Flask proves to be a flexible and lightweight web framework that simplifies database integration and offers built-in features for creating dynamic websites. By following a comprehensive development and deployment process, you can build a secure, feature-rich shopping site that meets user needs and stands out in the competitive online marketplace. "
    }, {
    "id": 12,
    "url": "http://localhost:4000/covid/",
    "title": "Coronavirus Data Analysis",
    "body": "2020/03/14 - Subtitle: COVID19 Spread SIR Model Simulation using Python Since the President declared a national emergency concerning the coronavirus disease, the world has been thrust into an unprecedented era marked by widespread uncertainty and profound challenges. The global response to the COVID-19 pandemic has emphasized the critical need for predictive models to understand and mitigate the spread of infectious diseases. The Susceptible-Infected-Recovered (SIR) model is commonly used in epidemiology to understand and predict the spread of infectious diseases within a population. In this context, the Susceptible-Infected-Recovered (SIR) model has emerged as a valuable tool in epidemiological research, offering insights into the dynamics of contagion within populations. Since the President declared a national emergency concerning the coronavirus disease, several months have been passed since the start of quarantine. The spread of COVID 19 seems going worse as more and more colleagues and friends are getting sick. I. Introduction A. Background on COVID-19 B. Importance of understanding disease spread C. Overview of SIR model and its significance D. Purpose of the essay: COVID-19 Spread SIR Model Simulation using Python II. SIR Model Explanation A. Components of the SIR model (Susceptible, Infected, Recovered) B. Differential equations governing the model C. Assumptions and limitations of the SIR model III. Python as a Simulation Tool A. Introduction to Python programming language B. Importance of simulation in epidemiology C. Brief overview of Python libraries for scientific computing (e. g. , NumPy, Matplotlib) IV. Data Collection and Preprocessing A. Gathering COVID-19 data for simulation B. Cleaning and formatting data for input into the SIR model V. Implementing the SIR Model in Python A. Coding the SIR model equations B. Incorporating relevant parameters and initial conditions C. Running simulations and obtaining results VI. Visualization of Simulation Results A. Creating graphs and plots using Matplotlib B. Analyzing and interpreting simulation outcomes C. Comparing simulated data with real-world COVID-19 trends VII. Validation and Sensitivity Analysis A. Validating the SIR model results against actual COVID-19 data B. Performing sensitivity analysis to assess the impact of parameter variations C. Discussing the implications of the findings VIII. Challenges and Limitations A. Addressing challenges in modeling disease spread B. Recognizing limitations of the SIR model and its assumptions C. Suggestions for improving future simulations IX. Conclusion A. Summarizing key findings and insights B. Emphasizing the importance of simulation in understanding COVID-19 spread C. Concluding remarks on the role of Python in epidemiological research X. References A. Citing relevant literature and sources B. Listing Python libraries and resources used in the simulation XI. Appendices A. Code snippets for implementing the SIR model in Python B. Additional graphs and charts for in-depth analysis C. Any supplementary information or data used in the essay In this time huge changes have occurred since I first started this blog in early February. My initial goal was to create a source that would collect total number of cases worldwide since the most updated information was not easily available to the public. However, the current situation has now changed, and there are more reliable resources that are documenting the spread of COVID-19 and disseminating the research in clear graphs. However, if you like to try my code, I have presented it below (output is in json format). def predict(c):	post_url =  https://covid19. who. int/page-data/region/wpro/country/cn/page-data. json 	try:		res = re. get(post_url, timeout=10)		status_code = res. status_code	except re. exceptions. ConnectionError:		status_code = 'CONNECTION ERROR'	if status_code != 200:		print ('___'*20, status_code)		return 'STATUS ERROR'		json_data = json. loads(res. json()['result']['data']['countryGroup']['data'])	cols = ['date','_']+[x['name'] for x in json_data['metrics']]	df = pd. DataFrame(data=json_data['rows'], columns=cols)	df['date'] = pd. to_datetime(df['date'],utc=True,unit='ms'). dt. date	df. drop(columns=['_'],inplace=True)	#	print (df. describe(). T) #	print (df. loc[df['Deaths']&lt;0]) # abnormal data neg daily death 	df = df. loc[df['Deaths']&gt;0]	print (df) predict(c='china') Although the situation with the coronavirus seems to be under control in China, the number of individuals infected in the US is rapidly increasing. As of March 31st 2020, the confirmed cases in the US reached around 188,530 and some officials are predicting that the number of deaths may reach up to 240,000.  Figure 1 (a). Total confirmed cases in US, China, Italy                       (b). Log scale In Figure 1, we can observe the progression of the coronavirus outbreak in three countries, each in a different phase. China has managed to control the spread of the virus, with the number of cases stabilizing after day 30. Italy’s rate of infection has slowed down and could potentially follow China’s trend (as indicated by the green line). In the US, we are still in the midst of a growing phase, but there is a glimmer of hope as further calculations suggest a possible slowdown (as indicated by the tip of the blue line). Figure 1b, which uses a log scale, demonstrates that the US and China have a higher infection rate due to their high population density. The slope of the graph in Figure 1b is calculated every 14 days using linear regression, and is obtained by breaking the data into smaller sections and taking the maximum slope. Analysis: Figure 1(a) shows that US data (blue line) probably follows an exponential trend ain the early stage, as also shown in the log Figure 1(b). Whereas, the green curve (China) looks more like a sigmoid function. So let’s first try to predict the infection population I(t) with a simple exponential model, defining I as infection population with an assumption that each patient infects A number of new people every day. This can be tested by by plotting log(y) against t - Figure 1(b): [\begin{equation}I = I_{0}\times A^{t}\end{equation}] where t is number of days since \(I_{0}\) cases. This model only fits the blue line, and does not fit the trend of the yellow or green line at later stage. After doing some research on spread of infectious diseases, it seems an SIR-model might be a better model. The model consists of three parts: infected population, susceptible population and recovered/immune population. As I am trying to model the total of confirmed cases, not the active cases, for simplicity I will I can drop the recovery aspect of the model. This greatly simplifies the math, allowing to simply solve an ordinary differential equation. To further simplify this, let’s define population as 1 and I(t) is the percentage of population that is infected and S(t) is the proportion of susceptible population. Then we get: [\begin{equation} S(t)+I(t) = 1\end{equation}] Define \(\beta\) as transmission rate. Upon further reflection, the rate of infection should be affected by both infection and susceptible population.  To consider two extreme cases,  when s = 1 , i = 0, the transmission rate, β = 0 when s = 0 , i = 1, everyone is infected and no more new cases, β=0If we divide both side by \(\delta t\), we can get: [\frac{dI}{dt} = - \frac{dS}{dt} = \beta S I] This is equation is an ODE function, and we can solve it using scipy package. In fact, turns out that this function \(\begin{equation}\frac{dI}{dt} = \beta I \cdot (1-I)\end{equation}\) is the derivate of sigmoid function [\begin{equation}I = \frac{1}{1+e^{-t}}\end{equation}] Prediction using sigmoid function: The model predicts the total confirmed cases next 30 days. For example, given data on March 1st, the model estimates about 172172 confirmed cases in Italy at the end of April. Thanks for reading ! "
    }, {
    "id": 13,
    "url": "http://localhost:4000/anime_recommender/",
    "title": "Anime Recommender",
    "body": "2019/05/28 - In recent years, anime has become increasingly popular worldwide, and the number of anime available online has grown exponentially. With so many options to choose from, it can be challenging to select the perfect one to suit their unique tastes. To address this issue, machine learning techniques has been used to develop personalized anime recommendation systems. By analyzing an individual’s viewing history and preferences, these systems can suggest anime titles that are more likely to appeal to them, based on factors such as genre, themes, and style. In this way, anime fans can discover new shows that are tailored to their specific interests, and spend less time searching. This article explores the use of machine learning in anime recommendation systems and discusses how they can enhance the anime viewing experience. The process began by gathering data from MyAnimeList, a website dedicated to anime similar to IMDb. Over 5000 anime titles and user profiles were collected using Scrapy, Beautiful-soup and stored as JSON objects in MongoDB. The collected information includes names, descriptions, directors, vocal casts, theme songs, reviews, and more. Below is an example demonstrating the format of the collected data. Examples of data stored in MangoDB. {'_id': 'Yaiba', 'themesongs': [[' Yuuki ga Areba (勇気があれば)  by Kabuki Rocks (カブキロックス)',  ' Shinchigakunaki Tatakai! (神智学無き戦い!)  by Kabuki Rocks (カブキロックス)']], 'description': [ Kurogane Yaiba is a boy who doesn't want to become what any regular kid would: A samurai. That's why he undergoes a hard training with his father, knowing only the forest as his world. Then, one day, he is sent to Japan, where he has to deal with a whole new civilized reality, meeting the Mine family, the evil Onimaru and even the legendary Musashi, having lots of dangerous adventures, becoming stronger everyday. (Source: ANN, edited)  ], 'reviews': ['which are really stupid but it all succeds in tickling us!!the storycharacter and enjoyment is quite okwell i personally disliked the op and ed and art also seems quite ok {not many cute girls :( }its a lot of fun overall the series i ll definately say give 1 shot only to the 1st epi!!!ull automatically get hooked to the series atleast i did !well i hope u liked my review plz ratemy 1st reviewread more'], 'img_url': 'https://myanimelist. cdn-dena. com/images/anime/5/71953. jpg'}The table illustrates an example of a user’s watched history, and the accompanying table displays ratings for 5 different anime given by 5 distinct users. The ratings are on a scale of 1 to 10, with 10 representing the highest level of favorability.       Anime/score   User1   User2   User3   User4   User5         Fairy Tail   10   8   10   5           Kimi no na wa   10   9           4       No game no life   9       9               Tokyo Ghoul   7       9               One Piece               10   9   How does this recommender system works ?: Broadly speaking, there are two common algorithms used in the recommendation systems, collaborative filtering and content-based filtering. Collaborative filtering works by analyzing the viewing histories and ratings of multiple users. The system identifies users who have similar viewing and rating patterns and groups them into clusters. It then makes recommendations based on the preferences of users in the same cluster. For example, if many Marvel fans has enjoyed Tom and Jerry in their past, the system will likely recommend Tom and Jerry to those Marvel fans who has not watched it yet. In other words, it makes predictions based on the response of other users who share similar tastes. Content-based filtering, on the other hand, makes recommendations based on the content of the anime themselves. This approach involves analyzing the attributes, such as genre, theme, plot, character and story background,. The system then recommends anime to users based on their preferences for these attributes. For example, if a user enjoys watching romantic comedies with high school settings, the system will search for anime with similar attributes. Both methods have their pros and cons. A major appeal of collaborative filtering is its flexibility in dealing with various data aspects. Collaborative filtering requires an active user data base with effective rating system and it does not work well with new user profiles or new anime with no ratings or reviews, known as cold start problem. It also relies heavily on the availability of user data, which can lead to sparsity and bias in data. A content based filtering is more friendly to new anime but is more exclusive to users’ own experience, and does not consider social factors such as popularity. A more effective solution would be a hybrid system that combines both methods. While there are many blogs online discussing these two methods, few dive into how they work in practice. To begin with, it is important to understand the concept of the cold start problem and how it arises. We can conceptualize the rating system as a matrix, denoted as Rrating, where the matrix contains the scores of all anime titles rated by all users. In our particular case, the matrix size will be 5000 shows by 5000 users, and each row will represent the ratings given by a user, with a value of 0 indicating that the user has not watched the anime. [[ 10 0  0  0  8  9  0  9  0  0  10 0  5  0  10  . . . ] [ 0  0  0  9  0  0  0  8  0  0  4  0  0  0  8  . . . ] [ 9  0  0  0  0  8  9  0  0  0  0  0  0  0  0  . . . ] [ 0  3  1  0  0  0  0  0  7  3  9  0  0  0  0  . . . ] [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  . . . ]]Let’s take a look at Row 5, which represents a new user. Typically, most users will have only watched a small fraction of the thousands of anime titles available on the platform. Assuming an average user spends about one hour per week on anime and each show has around 12 episodes of 20 minutes each, that would amount to roughly 152 hours per year, or about 20 shows per year. Consequently, the rating matrix will be extremely sparse, with most elements being zero. As the platform expands and more users join, the sparsity problem will continue to grow more severe. Figure 1. Hybrid recommender work flow. A layout structure of my code. A: Collaborative Filtering WorkflowThe following image displays a user-item interaction matrix obtained from the ratings of six shows given by six users. Traditional collaborative filtering include measuring user similarity by calculating Pearson correlation or cosine similarity between normalized user vectors. Then combine the weighted average scores given by neighbors to estimate user’s score on the unseen show. A modern solution called matrix factorization, initially introduced by Simon Funk in 2006 in the Netflix Prize competition, has a better approach in handling this user-item matrix.  Instead of directly computing similarity between users, matrix factorization transforms the original matrix into two lower-dimensional matrices - one representing users and the other representing items using technique called singular value decomposition (SVD). These lower-dimensional matrices capture the latent factors or features that determine the user’s preference for a particular item, and can be used to predict missing ratings. Matrix factorization can provide more accurate predictions and is more scalable than traditional collaborative filtering methods. Figure 2. User-item matrix decomposition \(R=\begin{bmatrix}5 &amp; 3 &amp; 0 &amp; 1 \\4 &amp; 0 &amp; 0&amp; 1\\1 &amp; 1 &amp; 0&amp; 5\\0 &amp; 1 &amp; 5&amp; 4\\\end{bmatrix}\rightarrow\;\;\;\; U \times V\;\;\rightarrow \;\;\hat{R}=\begin{bmatrix}4. 9&amp; 2. 5 &amp;2. 2 &amp;0. 9 \\4. 2 &amp; 0. 5 &amp; 3. 4&amp; 1\\1. 5 &amp; 0. 3 &amp; 4. 5&amp; 3. 9\\1. 1 &amp; 0. 9 &amp; 4. 9&amp; 3. 2\\\end{bmatrix}\)Rating matrix R can be expressed as the product of two lower-dimensional matrices: U (user matrix) and V (item matrix). Assume k=3 latent factors, which means we assume that there are three underlying factors that determine how users rate the items (e. g. action vs romance vs adventure, animation quality vs length vs vocal, etc). We initialize U and V randomly, then use gradient descent to optimize the matrices based on the mean squared error loss between the predicted ratings and actual ratings in R. We can then use these matrices to predict the missing values in R, by taking the dot product of the corresponding user and item vectors. B: Content-based Filtering workflowA way to understand content-based filtering is to see it as a classification problem, where the system identifies relevant features in the content that are highly correlated with the user’s preferences. Recommendations are then made by comparing the user’s profile to the content of each item in the collection. The inputs are descriptions of anime stories and the goal is to identify the key topics or themes present in the text by extracting and grouping related keywords. TF-IDF is a common method used for text extraction that calculates the importance of a specific word in a given document. Words that appear frequently across all documents, such as “people” or “place,” are given a lower importance than words that appear infrequently but suggest a specific theme or topic, such as “wolf,” “magic,” or “spirit”.  In this project, 30 different anime topics were identified by extracting the key phrases from all the story descriptions. For instance, Topic 7 is centered on solving crimes, with Detective Conan being the most representative example. Examples:  Topic #1: Special, release, air, recap, featureTopic #2: Earth, Planet, space, alien, shipTopic #3: High, school, junior, student, classmateTopic #4: Team, soccer, player, match, baseballTopic #5: Human, race, mankind, god, survive, extinct Topic #6: Magic, witch, magician, kingdom, wishTopic #7: Mystery, solve, appear, past, shadow, killTopic #8: Demon, king, hero, lord, seal, defeat, missionTopic #9: Love, feel, fall, relationship, confess, heart Then we can calculate an associated probability for each topic for a given anime.  For example       Anime/Topics   Topic 2   Topic 3   Topic 7   Topic 8   Topic 9         Fairy Tail   0. 01   0. 12   0. 00   0. 67   0. 00       Kimi no na wa   0. 11   0. 87   0. 02   0. 01   0. 54       No game no life   0. 35   0. 23   0. 34   0. 01   0. 35       Tokyo Ghoul   0. 02   0. 56   0. 76   0. 12   0. 00       One Piece   0. 05   0. 00   0. 00   0. 45   0. 00   We can determine a user’s preference for different topics by taking an average of all the anime topics they have watched in the past. To generate recommendations, we then calculate the cosine similarity between the user’s topic matrix and the anime topic matrix. Finally, we combine the results obtained from the two recommenders. Future Improvements: Incorporating contextual information, such as location, country, ethnicity, age to enhance the accuracy of recommendations by tailoring them to the user’s current situation. Use deep learning techniques, such as neural networks, to create more sophisticated models that can capture more complex patterns and relationships between users and items. Conclusion: Both methods have their strengths and limitations, and they can be combined to create a hybrid recommender system. The effectiveness of these methods can be further improved with advancements in natural language processing and machine learning techniques. Overall, recommender systems play a critical role in providing personalized experiences to users and are increasingly important in today’s digital landscape. Reference "
    }, {
    "id": 14,
    "url": "http://localhost:4000/illegal_mining/",
    "title": "Amazon Illegal Mining Detection",
    "body": "2019/01/18 - The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive practice has led to significant habitat destruction, biodiversity loss, and contributes to climate change. A key factor exacerbating this devastation is the sharp increase in illegal gold mining activities observed in the region over the past decade. Motivated by the soaring global gold price, individuals have engaged in unlawful mining practices within the Amazon rainforest, further exacerbating the environmental crisis. Traditional labor-intensive methods involving the hiring of local citizens to spot illegal activities have proven to be costly and time-consuming. Fortunately, advancements in satellite technology have opened new avenues for detecting illegal mining activities using satellite images and employing machine learning techniques like image classification. The primary objective of this project is to develop a system capable of identifying illegal mining activities among 16 other labels, including clouds, trees, ground, road, water, and more. To achieve this, I employed a dataset obtained from “Planet: Understanding the Amazon from Space. ” The dataset consisted of satellite images that were segmented into smaller chips and randomly assigned numerical names. Our aim was to generate accurate labels that effectively describe the content of each image and enable the identification of those chips associated with illegal mining. Neural networks were chosen as the preferred approach for this project due to their exceptional ability to recognize complex patterns and extract meaningful features from images. Start with basic Neural Network Model    Left Figure 1: Examples of image chips with labels.  Right Figure 2: Frequency distribution of labels The dataset consists of 70,000 image chips, each was associated with at least one label. The labels were categorized into three main groups:   atmospheric conditions,  common land uses and  rare land uses.  However, there were significant imbalances in the distribution of the labels, and a few labeling errors were present in the dataset. Among the labels, less than 1% represented categories such as artisanal mines, conventional mines, slash burn, and others. This highly imbalanced nature of the labels posed a challenge during the training process. The neural network had to learn to accurately classify and recognize the rare land use categories, despite having limited examples for these classes. Challenges of predicting rare used labelsMy initial attempt involved using a random forest with computed features such as haziness, image contrast, and RGB color channels. However, this approach proved unsuccessful, as the model struggled to differentiate the most basic weather conditions. Consequently, attention shifted towards a convolutional neural network (CNN), known for its prowess in capturing and extracting essential local features for pattern and object identification. While successful in discerning weather labels and common land uses, the CNN faced challenges in predicting rare land use labels, despite extensive training. Even after implementing oversampling techniques, the model not only failed to improve accuracy but also exacerbated the recall score, resulting in misclassifications of other labels as mining activities. In light of those challenges, I explored alternative approaches to improve model performance, particularly in predicting the 1% population representing rare land use categories.  Update computational power: use AWS GPU More advanced network designs Improve input image quality Rotate and flip images to increase the sample sizeUpdate computational powerTo harness the power of advanced hardware and accelerate the training process, an AWS GPU instance was utilized. The use of GPU computing combined with multi-processing technique allows for parallel processing and faster model training, enabling more efficient experimentation and optimization.  Additionally, cross-validation was implemented to continuously monitor the model’s performance across multiple iterations, ensuring that overfitting issues were identified and addressed promptly. Advanced network designs - DenseNetTo further improve the model’s performance and accuracy, a customized neural net model was implemented using DenseNet architecture. This deep neural network design introduces dense connections between layers, allowing for direct information flow and gradient propagation throughout the network. This architecture alleviates the vanishing gradient problem, encourages feature reuse and leads to better parameter efficiency and faster convergence during training. The Python package named Keras was employed to create and connect different neural network layers, with binary cross-entropy serving as the loss function. [\text{Binary Crossentropy } = -\sum y_i log({\hat{y}}_i)+(1-y_i)log({1-\hat{y}}_i)] , where y is the true probability and ŷ is the predicted probability. The binary cross-entropy loss function is particularly effective in multi-class problems. In this context, let’s consider a three-class problem with $y_A=1, y_B=1, y_C=0$.  Suppose the model predicts $ŷ_A=0. 4, ŷ_B=0. 7, ŷ_C=0. 1$ The loss is calculated as -(log(0. 4)+log(0. 7)+log(0. 9)) = 0. 60. As the predicted y of A and B approaches the correct answer 1, the loss function becomes small. Conversely, if predicted probability of A is close to zero, the loss becomes a large number. Same applies to $y_C$ when it approaches to 0. As a result, the new model benefits from requiring fewer parameters and faster convergence compared to CNN. It accelerates the training process and mitigates overfitting risk. This approach encourages the model to learn more robust and generalized features from the data, ultimately improving performance and accuracy. Improve image qualitySatellite images often suffer from darkness and blurriness caused by atmospheric turbulence. However, by mitigating the effects of haze and improving image quality, we can enhance the performance of neural networks. Leveraging my understanding of atmospheric physics and conducting research on Google Scholars, I developed a dehaze function based on a paper to address this challenge. Haze results from the scattering of light in the atmosphere before it reaches the camera. To estimate the intensity of scattered light, a constant value is derived by approximating the maximum pixel intensity within the darkest RGB channel. By utilizing OpenCV to convert images to a colorspace matrix and calculating the haze constant, we can restore the image by subtracting this value. As a result, the dehazed image appears brighter and exhibits better contrast, improving the quality of input data for neural networks. Figure 3: Formation of a hazy image. Camera = Direct attenuation+Airlight Figure4 : Examples of satellite image chips before and after haze removal The impact of the dehazing process is evident, as it leads to a substantial improvement in image quality across most samples. Upon the application of the haze removal function, the image undergoes a remarkable transformation. The contrast is restored, bringing out more vivid and distinct features. The sharpness of objects is improved, and details that were previously obscured by haze become more visible. Colors, regaining vibrancy and accuracy, contribute to a clearer and more faithful representation of the scene. Overall, the haze removal function significantly elevates the image visibility and quality by reducing the impact of atmospheric interference. The choice of F2 score as the performance metric is deliberate due to the uneven distribution of label classes, where recall takes precedence over precision. Recall measures the ability to capture all positive instances correctly. In the specific context of identifying labels associated with mining, a high recall implies the model’s effectiveness in detecting actual mining instances, thereby minimizing the occurrence of false negatives (missed mining instances). The F2 score, by assigning greater weight to recall, ensures that the model adeptly identifies as many positive instances as possible while maintaining a reasonable level of precision. This strategic emphasis aligns with the objective of optimizing performance in scenarios where missing positive instances carries a higher cost than false positives.  Figure 5: F2 score before (purple) and after (blue) the haze removal function. Notice it dramatically improve the results for rare land use labels Work Flow Model performanceAfter training the DenseNet model on the dehazed image sets, the model was saved as “b01_dense121. h5”. This trained model was then utilized to generate labels for the previously unlabelled test set images. The model successfully identified several test photo chips as illegal mines, providing valuable insights into the presence of illegal mining activities in the dataset. Figure 6. Examples from the test data that were marked as illegal mines with labels generated by the DenseNet model. In summary, the project focused on developing a model to detect illegal mining and rare land use cases using satellite images. Several approaches were employed to address the challenges of imbalanced labels, low image quality, and underfitting. These approaches included improving computational power, rotating images of rare land use categories, using more advanced network designs like DenseNet, and enhancing input image quality through haze removal. The implementation of these approaches resulted in significant improvements in both precision and recall, particularly for the rare land use cases. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        
<div class="container">
<div class="jumbotron jumbotron-fluid mb-3 pl-0 pt-0 pb-0 bg-white position-relative">
		<div class="h-100 tofront">
			<div class="row  justify-content-between ">
				<div class=" col-md-6  pr-0 pr-md-4 pt-4 pb-4 align-self-center">
					<p class="text-uppercase font-weight-bold">
                        <span class="catlist">
						
                          <a class="sscroll text-danger" href="/categories.html#data scraping">data scraping</a><span class="sep">, </span>
                        
                        </span>
					</p>
					<h1 class="display-4 mb-4 article-headline">Coronavirus Data Analysis</h1>
					<div class="d-flex align-items-center">
                        
                        <img class="rounded-circle" src="/assets/images/avatar.jpeg" alt="Penny" width="70"/>
                        
						<small class="ml-3"> Penny
                            <span class="text-muted d-block mt-1">Mar 14, 2020 · <span class="reading-time">
  
  
    7 mins read
  
</span>
    </span>
						</small>
					</div>
				</div>
                
				<div class="col-md-6 pr-0 align-self-center">
					<img class="rounded" src="https://safeharborsc.imgix.net/adobestock-339124602-converted-1731501992.png" alt="Coronavirus Data Analysis">
				</div>
                
			</div>
		</div>
	</div>
</div>




<div class="container-lg pt-4 pb-4">
	<div class="row justify-content-center">


        <!-- Share -->
		<div class="col-lg-1">

		</div>


		<div class="col-md-12 col-lg-8">

            <!-- Article -->
			<article class="article-post">
			<p>Subtitle: COVID19 Spread SIR Model Simulation using Python</p>

<p>Since the President declared a national emergency concerning the coronavirus disease, the world has been thrust into an unprecedented era marked by widespread uncertainty and profound challenges. The global response to the COVID-19 pandemic has emphasized the critical need for predictive models to understand and mitigate the spread of infectious diseases.</p>

<p>The Susceptible-Infected-Recovered (SIR) model is commonly used in epidemiology to understand and predict the spread of infectious diseases within a population.</p>

<p>In this context, the Susceptible-Infected-Recovered (SIR) model has emerged as a valuable tool in epidemiological research, offering insights into the dynamics of contagion within populations.</p>

<p>Since the President declared a national emergency concerning the coronavirus disease, several months have been passed since the start of quarantine. The spread of COVID 19 seems going worse as more and more colleagues and friends are getting sick.</p>

<p>I. Introduction A. Background on COVID-19 B. Importance of understanding disease spread C. Overview of SIR model and its significance D. Purpose of the essay: COVID-19 Spread SIR Model Simulation using Python</p>

<p>II. SIR Model Explanation A. Components of the SIR model (Susceptible, Infected, Recovered) B. Differential equations governing the model C. Assumptions and limitations of the SIR model</p>

<p>III. Python as a Simulation Tool A. Introduction to Python programming language B. Importance of simulation in epidemiology C. Brief overview of Python libraries for scientific computing (e.g., NumPy, Matplotlib)</p>

<p>IV. Data Collection and Preprocessing A. Gathering COVID-19 data for simulation B. Cleaning and formatting data for input into the SIR model</p>

<p>V. Implementing the SIR Model in Python A. Coding the SIR model equations B. Incorporating relevant parameters and initial conditions C. Running simulations and obtaining results</p>

<p>VI. Visualization of Simulation Results A. Creating graphs and plots using Matplotlib B. Analyzing and interpreting simulation outcomes C. Comparing simulated data with real-world COVID-19 trends</p>

<p>VII. Validation and Sensitivity Analysis A. Validating the SIR model results against actual COVID-19 data B. Performing sensitivity analysis to assess the impact of parameter variations C. Discussing the implications of the findings</p>

<p>VIII. Challenges and Limitations A. Addressing challenges in modeling disease spread B. Recognizing limitations of the SIR model and its assumptions C. Suggestions for improving future simulations</p>

<p>IX. Conclusion A. Summarizing key findings and insights B. Emphasizing the importance of simulation in understanding COVID-19 spread C. Concluding remarks on the role of Python in epidemiological research</p>

<p>X. References A. Citing relevant literature and sources B. Listing Python libraries and resources used in the simulation</p>

<p>XI. Appendices A. Code snippets for implementing the SIR model in Python B. Additional graphs and charts for in-depth analysis C. Any supplementary information or data used in the essay</p>

<p>In this time huge changes have occurred since I first started this blog in early February. My initial goal was to create a source that would collect total number of cases worldwide since the most updated information was not easily available to the public. However, the current situation has now changed, and there are more reliable resources that are documenting the spread of COVID-19 and disseminating the research in clear graphs. However, if you like to try my code, I have presented it below (output is in json format).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">c</span><span class="p">):</span>
	<span class="n">post_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://covid19.who.int/page-data/region/wpro/country/cn/page-data.json</span><span class="sh">"</span>
	<span class="k">try</span><span class="p">:</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">post_url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
		<span class="n">status_code</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">status_code</span>
	<span class="k">except</span> <span class="n">re</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="nb">ConnectionError</span><span class="p">:</span>
		<span class="n">status_code</span> <span class="o">=</span> <span class="sh">'</span><span class="s">CONNECTION ERROR</span><span class="sh">'</span>
	<span class="k">if</span> <span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
		<span class="nf">print </span><span class="p">(</span><span class="sh">'</span><span class="s">___</span><span class="sh">'</span><span class="o">*</span><span class="mi">20</span><span class="p">,</span> <span class="n">status_code</span><span class="p">)</span>
		<span class="k">return</span> <span class="sh">'</span><span class="s">STATUS ERROR</span><span class="sh">'</span>
	
	<span class="n">json_data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="nf">json</span><span class="p">()[</span><span class="sh">'</span><span class="s">result</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">countryGroup</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">])</span>
	<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">json_data</span><span class="p">[</span><span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">]]</span>
	<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">json_data</span><span class="p">[</span><span class="sh">'</span><span class="s">rows</span><span class="sh">'</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span>
	<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">],</span><span class="n">utc</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">unit</span><span class="o">=</span><span class="sh">'</span><span class="s">ms</span><span class="sh">'</span><span class="p">).</span><span class="n">dt</span><span class="p">.</span><span class="n">date</span>
	<span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">],</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>	
<span class="c1">#	print (df.describe().T) 
#	print (df.loc[df['Deaths']&lt;0]) # abnormal data neg daily death 
</span>	<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Deaths</span><span class="sh">'</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">]</span>
	<span class="nf">print </span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
 
<span class="nf">predict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">china</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://media.licdn.com/dms/image/C4D12AQHYhvRt4Uvx4w/article-inline_image-shrink_1500_2232/0/1585841101969?e=1686182400&amp;v=beta&amp;t=ZbDvR8lghy1WX-asXAbBmCO7YPrBzVquecINlbjaB7s" /></p>

<p><img src="https://media.licdn.com/dms/image/C4D12AQFkpvj32zpmlg/article-inline_image-shrink_1000_1488/0/1585841113925?e=1686182400&amp;v=beta&amp;t=7G-Rh-lIOyXqSLiGnuz4jDi9VXNflaIzard356vus8Q" /></p>

<p>Although the situation with the coronavirus seems to be under control in China, the number of individuals infected in the US is rapidly increasing. As of March 31st 2020, the confirmed cases in the US reached around 188,530 and some officials are predicting that the number of deaths may reach up to 240,000.</p>

<p><img src="https://media.licdn.com/dms/image/C4E12AQFMq3auItOSeA/article-inline_image-shrink_1000_1488/0/1585839808611?e=1686182400&amp;v=beta&amp;t=V6fHTzkIhoSSxyiZrPQyNdZm709QP9TE-p50OLIF9xo" width="800" alt="corona_tot" /></p>

<p>Figure 1 (a). Total confirmed cases in US, China, Italy                                             (b). Log scale</p>

<p>In Figure 1, we can observe the progression of the coronavirus outbreak in three countries, each in a different phase. China has managed to control the spread of the virus, with the number of cases stabilizing after day 30. Italy’s rate of infection has slowed down and could potentially follow China’s trend (as indicated by the green line). In the US, we are still in the midst of a growing phase, but there is a glimmer of hope as further calculations suggest a possible slowdown (as indicated by the tip of the blue line). Figure 1b, which uses a log scale, demonstrates that the US and China have a higher infection rate due to their high population density. The slope of the graph in Figure 1b is calculated every 14 days using linear regression, and is obtained by breaking the data into smaller sections and taking the maximum slope.</p>

<h4 id="analysis"><strong>Analysis</strong></h4>

<p>Figure 1(a) shows that US data (blue line) probably follows an exponential trend ain the early stage, as also shown in the log Figure 1(b). Whereas, the green curve (China) looks more like a sigmoid function. So let’s first try to predict the infection population <strong>I(t)</strong> with a simple exponential model, defining <strong>I</strong> as infection population with an assumption that each patient infects <strong>A</strong> number of new people every day. This can be tested by by plotting log(y) against t - Figure 1(b):</p>

\[\begin{equation}
I = I_{0}\times A^{t}
\end{equation}\]

<p>where t is number of days since \(I_{0}\) cases.</p>

<p>This model only fits the blue line, and does not fit the trend of the yellow or green line at later stage. After doing some research on spread of infectious diseases, it seems an SIR-model might be a better model. The model consists of three parts: infected population, susceptible population and recovered/immune population. As I am trying to model the total of confirmed cases, not the active cases, for simplicity I will I can drop the recovery aspect of the model. This greatly simplifies the math, allowing to simply solve an ordinary differential equation. To further simplify this, let’s define population as 1 and <strong>I(t)</strong> is the percentage of population that is infected and <strong>S(t)</strong> is the proportion of susceptible population. Then we get:</p>

\[\begin{equation} 
S(t)+I(t) = 1
\end{equation}\]

<p>Define \(\beta\)  as  <strong>transmission rate</strong>. Upon further reflection, the rate of infection should be affected by both infection and susceptible population.  To consider two extreme cases,</p>

<ul>
  <li>when s = 1 , i = 0, the transmission rate, β = 0</li>
  <li>when s = 0 , i = 1, everyone is infected and no more new cases, β=0</li>
</ul>

<p>If we divide both side by \(\delta t\), we can get:</p>

\[\frac{dI}{dt} = - \frac{dS}{dt} = \beta S I\]

<p>This is equation is an ODE function, and we can solve it using scipy package.</p>

<p>In fact, turns out that this function 
\(\begin{equation}
\frac{dI}{dt} = \beta I \cdot (1-I)
\end{equation}\)</p>

<p>is the derivate of sigmoid function</p>

\[\begin{equation}
I = \frac{1}{1+e^{-t}}
\end{equation}\]

<h4 id="prediction-using-sigmoid-function"><strong>Prediction using sigmoid function</strong></h4>

<p>The model predicts the total confirmed cases next 30 days. For example, given data on March 1st, the model estimates about 172172 confirmed cases in Italy at the end of April.</p>

<p>Thanks for reading !</p>


			</article>

			<!-- Tags -->
			<div class="mb-4">
				<span class="taglist">
				
				</span>
			</div>


             <!-- Author Box -->
                
				<div class="row mt-5">
					<div class="col-md-2 align-self-center">
                         
                        <img class="rounded-circle" src="/assets/images/avatar.jpeg" alt="Penny" width="90"/>
                        
					</div>
					<div class="col-md-10">
                        <h5 class="font-weight-bold">Written by Penny</h5>
						Data scienctist, love to explore new ideas
					</div>
				</div>
                


		</div>


	</div>
</div>


<!-- Aletbar Prev/Next -->
<div class="alertbar">
    <div class="container">
        <div class="row prevnextlinks small font-weight-bold">
          
            <div class="col-md-6 rightborder pl-0">
				<i class='fas fa-angle-left fa-lg' style='color:#6610f2'></i>
                <a class="text-dark" href="/anime_recommender/"> Anime Recommender</a>
            </div>
          
          
            <div class="col-md-6 text-right pr-0">
                <a class="text-dark" href="/Ecommerce/"> Building a Dynamic E-commerce Website for Small Businesses </a>
				<i class='fas fa-angle-right fa-lg' style='color:#6610f2'></i>

            </div>
          
        </div>
    </div>
</div>

    </main>



    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Penny</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/mumuxi15/02-Data-Science-Project"><i class="fab fa-github"></i> Github Projects</a>

            </div>
            <div>
                Made with Jekyll Theme
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
