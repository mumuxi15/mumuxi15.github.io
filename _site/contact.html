<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Contact | Penny Pan</title>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Contact | Penny Pan</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Contact" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Have a good day ~" />
<meta property="og:description" content="Have a good day ~" />
<link rel="canonical" href="http://localhost:4000/contact.html" />
<meta property="og:url" content="http://localhost:4000/contact.html" />
<meta property="og:site_name" content="Penny Pan" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Contact" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Have a good day ~","headline":"Contact","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"url":"http://localhost:4000/contact.html"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Penny Pan</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": ""
    }, {
    "id": 2,
    "url": "http://localhost:4000/author-penny.html",
    "title": "Penny",
    "body": "                    Penny:                 Data scienctist, love to explore new ideas                                   Posts by Penny:                   		E-commerce web	: 		  Introduction to building website	 			In 								Jun 06, 2021						            		Coronavirus Data Analysis	: 		  Subtitle: COVID19 Spread Simulation using Python	 			In 								Mar 14, 2020						            		Anime Recommender	: 		  In recent years, anime has become increasingly popular worldwide, and the number of anime available online has grown exponentially. With so many options to choose from, it can be cha. . . 	 			In 				supervised, 								May 28, 2019						            		Amazon Illegal Mining Detection	: 		  The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive . . . 	 			In 				classification, 				supervised, 								Jan 18, 2019						        "
    }, {
    "id": 3,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                             Penny :       (View Posts)      Data scienctist, love to explore new ideas                                             "
    }, {
    "id": 4,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories               classification:                                  		Amazon Illegal Mining Detection	: 		  The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive . . . 	 			In 				classification, 				supervised, 								Jan 18, 2019						                              supervised:                                  		Anime Recommender	: 		  In recent years, anime has become increasingly popular worldwide, and the number of anime available online has grown exponentially. With so many options to choose from, it can be cha. . . 	 			In 				supervised, 								May 28, 2019						                                 		Amazon Illegal Mining Detection	: 		  The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive . . . 	 			In 				classification, 				supervised, 								Jan 18, 2019						                                             Featured:    				                                          Anime Recommender                          In                     supervised,                                                                   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to Penny Pan. We will reply as soon as possible!   "
    }, {
    "id": 6,
    "url": "http://localhost:4000/",
    "title": "Free Jekyll Theme",
    "body": "{% if page. url ==  /  %}    {% assign latest_post = site. posts[0] %}        &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               {{ latest_post. date | date: '%b %d, %Y' }}                      {%- assign second_post = site. posts[1] -%}            {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                            {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                            {{ second_post. date | date: '%b %d, %Y' }}                                {%- assign third_post = site. posts[2] -%}            {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                            {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                            {{ third_post. date | date: '%b %d, %Y' }}                                {%- assign fourth_post = site. posts[3] -%}            {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                            {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                            {{ fourth_post. date | date: '%b %d, %Y' }}                          {% for post in site. posts %}{% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More                        {% endif %}{% endfor %}{% endif %}       All Stories:     {% for post in paginator. posts %}      {% include main-loop-card. html %}    {% endfor %}               {% if paginator. total_pages &gt; 1 %}             {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 7,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 8,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags    {% for tag in site. tags %}    {{ tag[0] }}:     {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}      {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}              {% include sidebar-featured. html %}    "
    }, {
    "id": 9,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 10,
    "url": "http://localhost:4000/Ecommerce/",
    "title": "E-commerce web",
    "body": "2021/06/06 - Introduction to building website The ongoing COVID-19 pandemic has dramatically changed the way people shop and do business. With more and more consumers preferring to shop online rather than in-person, small business owners have an opportunity to expand their customer base and increase their revenue by setting up a simple e-commerce website with a database. A dynamic website can help small businesses create an engaging and interactive online presence that attracts new customers and allows existing customers to easily place orders and interact with the business. Setting up a website involves several key steps, including choosing a web hosting provider, selecting a database management system, designing the website’s front-end, creating the back-end infrastructure, and integrating the database with the website. In this introduction, we will guide you through the basic steps involved in setting up a dynamic website for a small business with a database, highlighting important considerations and best practices along the way. With the right tools and knowledge, small business owners can create a successful online presence that helps them navigate the challenges of the pandemic and thrive in a digital world. Many prefer to use Flask due to its ease of use and lightweight nature. Flask makes it easy to start building a web application without having to deal with the complexities of a full-stack web framework. As a lightweight micro-framework, it provides the flexibility to build a custom web application tailored to the specific needs of the project. 1. Plan and Design: To start a project, it is always important to plan out the features and functionality you want to include, including the color theme, payment gateway integration and etc. Popular payment gateways include PayPal, Stripe, Braintree and Square, all those have API access and allow backtesting. 2. Set up virtual environment: Starting your project with a virtual environment tool virtualenv, helps to keep project dependencies organized and prevents conflicts between different projects. Once you have created a virtual environment, you can activate it by running the command. Useful packages include Flask-login, Flask-admin, Flask-Security, Flask-dance (for third party login such as google), Flask_sqlalchemy and etc. virtualenv -p python3 &lt;desired-path&gt;source &lt;venv&gt;/bin/activate3. Build Backend - Database: Ask yourself a few questions before you start building the backend:  What information I want to show to the client ? What information I want to collect from the website ?Drawing a quick database schema can also be very helpful in visualizing the data model and identifying the relationships between different entities. Picture below is an example of the standard schema and the grey lines shows how the tables are linked. The “order_detail” table share a common key “order_id” with the “orders” table. This would allow you to link specific items in an order to their corresponding product records, so you can accurately track inventory levels and update product availability as items are sold.  4. Build Frontend: You can quickly get the website to running by using Jinja to dynamically generate HTML pages based on data from your database and user input. This allows you to create customized web pages that can display product listings, shopping carts, checkout forms, and other essential features of an e-commerce website. A basic set up looks like the example below. Project Tree Example using Blueprints e_commerce_web├── /application│  ├── __init__. py│  ├── /auth│  │  ├── auth_routes. py│  │  └── /templates│  ├── /main│  │  ├── main_routes. py│  │  └── /templates│  ├── /static│  └── /templates│    ├── layout. html│    ├── home. html│    └── login. html├── requirements. txt└── wsgi. py└── venvThe function create_app() is used to build a Flask app __int__. py # . . . #def create_app():	app = Flask(__name__)	app. config['SECRET_KEY'] = ''	app. config['SQLALCHEMY_DATABASE_URI'] = db_path	app. config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False	app. config['OAUTH_CONFIG'] = {''}	db. init_app(app)	login_manager = LoginManager()	login_manager. login_view = 'auth. login'	login_manager. init_app(app)	from . models import User	admin = Admin(app,index_view=CustomAdminIndexView())	#. . . # 	@login_manager. user_loader	def load_user(user_id):	# use user_id in the query for the user		return User. query. get(int(user_id))	# blueprint for auth routes in our app	from . auth import auth as auth_blueprint	app. register_blueprint(auth_blueprint)	# blueprint for non-auth parts of app	from . main import main as main_blueprint	app. register_blueprint(main_blueprint) 	# register google / facebook login	init_oauth_providers(app)	return appReference: Flask-login document Flask-login github demo Organzing Flask Apps with Blueprints 5. Add e-commerce features: As the website grows, you can add additional features and functionalities to enhance the user experience. For instance, payment gateways, search functionality, product reviews, wish lists, and social media integration to increase user engagement and boost sales. With Flask’s modular design and scalability, you can add these features to the website as needed, making it a versatile and flexible tool for building an e-commerce website. In order to make a dynamic website, you need to have a basic understanding of JavaScript programming and be familiar with both client-side and server-side development. The main challenge of building a dynamic website is connecting the database to the website and passing data between them. It’s like building a bridge between two cities over the sea. One way to do this is by using HTML5 web storage to store shopping cart information and using AJAX to transfer data from the front-end to the back-end, where it can be stored in the database. If you’re not familiar with these technologies, I recommend watching some YouTube tutorials to learn more. JavaScript Shopping Cart Tutorial 6. Testing: This involves running different types of tests such as unit tests, integration tests, and end-to-end tests to verify the functionality of different components of the website. You can use testing frameworks such as Pytest and unittest to write and run tests for your website. Debugging any issues or errors that arise is also crucial to make sure that your website is functioning smoothly. Links to good articles on web testing.  unittesting 7. Deploy: Once you have tested your website and are confident that it works as expected, the next step is to deploy it to a production environment where it can be accessed by users. There are various hosting services available such as Heroku, AWS, and Google Cloud Platform that can be used to deploy your website. You will need to configure the hosting environment, set up the database and server, and deploy your code to the server. It is also important to consider issues such as scalability, security, and availability when deploying your website. I used pythoneverywhere. com to host my website. It is affordable and easy to use. It provides a web-based development environment and a platform for hosting web applications written in Python. With a paid account, you can host up to three web applications for $5 per month. When building a website, it’s important to ensure that the site is secure and the sensitive customer information such as names, addresses, and payment details are protected from potential hackers or other security threats. One way to accomplish this is to use encryption to protect the data as it is transmitted between the client and server. First, you’ll need to obtain an SSL (Secure Sockets Layer) certificate, which verifies the identity of your website and encrypts all data transmitted between the server and the client. You can obtain a SSL certificate from a trusted certificate authority such as Let’s Encrypt (FREE !!!) Once you have the certificate, configure the web server to use HTTPS instead of HTTP. HTTPS is a secure version of the HTTP protocol that encrypts all data transmitted between the client and server. In addition to using encryption, you should also take other security measures such as regularly updating your software and plugins, using strong passwords, and implementing measures such as two-factor authentication to protect your website from potential threats. It’s also a good idea to regularly back up your website to protect against data loss in the event of a security breach or other issue. 8. Google Search: If you want your website to appear in google search,    Verify the domain on google search console. You can open the console by typing in google   site:xxx. com      Complete the DNS verification as google requested     Buy a domain name.     Edit CNAME record in the DNS manage section. Here I used goDaddy as an example.  host: wwwpoints to: yourwebsitename. com Then go back to google console, search for your url in the URL inspection section and submit the index request. Google will perform several test on the website and if passed, you will receive a confirmation email from google. Overall, Flask is a flexible and lightweight web framework that simplifies database integration and offers a range of built-in features that can be customized and extended to create dynamic websites for small businesses during the pandemic. "
    }, {
    "id": 11,
    "url": "http://localhost:4000/covid/",
    "title": "Coronavirus Data Analysis",
    "body": "2020/03/14 - Subtitle: COVID19 Spread Simulation using Python In New York, almost a month has passed since the start of the quarantine. In this time huge changes have occurred since I first started this blog in early February. My initial goal was to create a source that would collect total number of cases worldwide since the most updated information was not easily available to the public. However, the current situation has now changed, and there are more reliable resources that are documenting the spread of COVID-19 and disseminating the research in clear graphs. However, if you like to try my code, I have presensted it below (output is in json format).  Although the situation with the coronavirus seems to be under control in China, the number of individuals infected in the US is rapidly increasing. As of March 31st 2020, the confirmed cases in the US reached around 188,530 and some officials are predicting that the number of deaths may reach up to 240,000.  Figure 1 (a). Total confirmed cases in US, China, Italy                       (b). Log scale In Figure 1, we can observe the progression of the coronavirus outbreak in three countries, each in a different phase. China has managed to control the spread of the virus, with the number of cases stabilizing after day 30. Italy’s rate of infection has slowed down and could potentially follow China’s trend (as indicated by the green line). In the US, we are still in the midst of a growing phase, but there is a glimmer of hope as further calculations suggest a possible slowdown (as indicated by the tip of the blue line). Figure 1b, which uses a log scale, demonstrates that the US and China have a higher infection rate due to their high population density. The slope of the graph in Figure 1b is calculated every 14 days using linear regression, and is obtained by breaking the data into smaller sections and taking the maximum slope. Analysis: Figure 1(a) shows that US data (blue line) probably follows an exponential trend ain the early stage, as also shown in the log Figure 1(b). Whereas, the green curve (China) looks more like a sigmoid function. So let’s first try to predict the infection population I(t) with a simple exponential model, defining I as infection population with an assumption that each patient infects A number of new people every day. This can be tested by by plotting log(y) against t - Figure 1(b): [\begin{equation}I = I_{0}\times A^{t}\end{equation}] where t is number of days since \(I_{0}\) cases. This model only fits the blue line, and does not fit the trend of the yellow or green line at later stage. After doing some research on spread of infectious diseases, it seems an SIR-model might be a better model. The model consists of three parts: infected population, susceptible population and recovered/immune population. As I am trying to model the total of confirmed cases, not the active cases, for simplicity I will I can drop the recovery aspect of the model. This greatly simplifies the math, allowing to simply solve an ordinary differential equation. To further simplify this, let’s define population as 1 and I(t) is the percentage of population that is infected and S(t) is the proportion of susceptible population. Then we get: [\begin{equation} S(t)+I(t) = 1\end{equation}] Define \(\beta\) as transmission rate. Upon further reflection, the rate of infection should be affected by both infection and susceptible population.  To consider two extreme cases,  when s = 1 , i = 0, the transmission rate, β = 0 when s = 0 , i = 1, everyone is infected and no more new cases, β=0If we divide both side by \(\delta t\), we can get: [\frac{dI}{dt} = - \frac{dS}{dt} = \beta S I] This is equation is an ODE function, and we can solve it using scipy package. In fact, turns out that this function \(\begin{equation}\frac{dI}{dt} = \beta I \cdot (1-I)\end{equation}\) is the derivate of sigmoid function [\begin{equation}I = \frac{1}{1+e^{-t}}\end{equation}] Prediction using sigmoid function: The model predicts the total confirmed cases next 30 days. For example, given data on March 1st, the model estimates about 172172 confirmed cases in Italy at the end of April. Thanks for reading ! "
    }, {
    "id": 12,
    "url": "http://localhost:4000/anime_recommender/",
    "title": "Anime Recommender",
    "body": "2019/05/28 - In recent years, anime has become increasingly popular worldwide, and the number of anime available online has grown exponentially. With so many options to choose from, it can be challenging to select the perfect one to suit their unique tastes. To address this issue, machine learning techniques has been used to develop personalized anime recommendation systems. By analyzing an individual’s viewing history and preferences, these systems can suggest anime titles that are more likely to appeal to them, based on factors such as genre, themes, and style. In this way, anime fans can discover new shows that are tailored to their specific interests, and spend less time searching. This article explores the use of machine learning in anime recommendation systems and discusses how they can enhance the anime viewing experience. The process began by gathering data from MyAnimeList, a website dedicated to anime similar to IMDb. Over 5000 anime titles and user profiles were collected using Scrapy, Beautiful-soup and stored as JSON objects in MongoDB. The collected information includes names, descriptions, directors, vocal casts, theme songs, reviews, and more. Below is an example demonstrating the format of the collected data. Examples of data stored in MangoDB. {'_id': 'Yaiba', 'themesongs': [[' Yuuki ga Areba (勇気があれば)  by Kabuki Rocks (カブキロックス)',  ' Shinchigakunaki Tatakai! (神智学無き戦い!)  by Kabuki Rocks (カブキロックス)']], 'description': [ Kurogane Yaiba is a boy who doesn't want to become what any regular kid would: A samurai. That's why he undergoes a hard training with his father, knowing only the forest as his world. Then, one day, he is sent to Japan, where he has to deal with a whole new civilized reality, meeting the Mine family, the evil Onimaru and even the legendary Musashi, having lots of dangerous adventures, becoming stronger everyday. (Source: ANN, edited)  ], 'reviews': ['which are really stupid but it all succeds in tickling us!!the storycharacter and enjoyment is quite okwell i personally disliked the op and ed and art also seems quite ok {not many cute girls :( }its a lot of fun overall the series i ll definately say give 1 shot only to the 1st epi!!!ull automatically get hooked to the series atleast i did !well i hope u liked my review plz ratemy 1st reviewread more'], 'img_url': 'https://myanimelist. cdn-dena. com/images/anime/5/71953. jpg'}The table illustrates an example of a user’s watched history, and the accompanying table displays ratings for 5 different anime given by 5 distinct users. The ratings are on a scale of 1 to 10, with 10 representing the highest level of favorability.       Anime/score   User1   User2   User3   User4   User5         Fairy Tail   10   8   10   5           Kimi no na wa   10   9           4       No game no life   9       9               Tokyo Ghoul   7       9               One Piece               10   9   How does this recommender system works ?: Broadly speaking, there are two common algorithms used in the recommendation systems, collaborative filtering and content-based filtering. Collaborative filtering works by analyzing the viewing histories and ratings of multiple users. The system identifies users who have similar viewing and rating patterns and groups them into clusters. It then makes recommendations based on the preferences of users in the same cluster. For example, if many Marvel fans has enjoyed Tom and Jerry in their past, the system will likely recommend Tom and Jerry to those Marvel fans who has not watched it yet. In other words, it makes predictions based on the response of other users who share similar tastes. Content-based filtering, on the other hand, makes recommendations based on the content of the anime themselves. This approach involves analyzing the attributes, such as genre, theme, plot, character and story background,. The system then recommends anime to users based on their preferences for these attributes. For example, if a user enjoys watching romantic comedies with high school settings, the system will search for anime with similar attributes. Both methods have their pros and cons. A major appeal of collaborative filtering is its flexibility in dealing with various data aspects. Collaborative filtering requires an active user data base with effective rating system and it does not work well with new user profiles or new anime with no ratings or reviews, known as cold start problem. It also relies heavily on the availability of user data, which can lead to sparsity and bias in data. A content based filtering is more friendly to new anime but is more exclusive to users’ own experience, and does not consider social factors such as popularity. A more effective solution would be a hybrid system that combines both methods. While there are many blogs online discussing these two methods, few dive into how they work in practice. To begin with, it is important to understand the concept of the cold start problem and how it arises. We can conceptualize the rating system as a matrix, denoted as Rrating, where the matrix contains the scores of all anime titles rated by all users. In our particular case, the matrix size will be 5000 shows by 5000 users, and each row will represent the ratings given by a user, with a value of 0 indicating that the user has not watched the anime. [[ 10 0  0  0  8  9  0  9  0  0  10 0  5  0  10  . . . ] [ 0  0  0  9  0  0  0  8  0  0  4  0  0  0  8  . . . ] [ 9  0  0  0  0  8  9  0  0  0  0  0  0  0  0  . . . ] [ 0  3  1  0  0  0  0  0  7  3  9  0  0  0  0  . . . ] [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  . . . ]]Let’s take a look at Row 5, which represents a new user. Typically, most users will have only watched a small fraction of the thousands of anime titles available on the platform. Assuming an average user spends about one hour per week on anime and each show has around 12 episodes of 20 minutes each, that would amount to roughly 152 hours per year, or about 20 shows per year. Consequently, the rating matrix will be extremely sparse, with most elements being zero. As the platform expands and more users join, the sparsity problem will continue to grow more severe. Figure 1. Hybrid recommender work flow. A layout structure of my code. A: Collaborative Filtering WorkflowThe following image displays a user-item interaction matrix obtained from the ratings of six shows given by six users. Traditional collaborative filtering include measuring user similarity by calculating Pearson correlation or cosine similarity between normalized user vectors. Then combine the weighted average scores given by neighbors to estimate user’s score on the unseen show. A modern solution called matrix factorization, initially introduced by Simon Funk in 2006 in the Netflix Prize competition, has a better approach in handling this user-item matrix.  Instead of directly computing similarity between users, matrix factorization transforms the original matrix into two lower-dimensional matrices - one representing users and the other representing items using technique called singular value decomposition (SVD). These lower-dimensional matrices capture the latent factors or features that determine the user’s preference for a particular item, and can be used to predict missing ratings. Matrix factorization can provide more accurate predictions and is more scalable than traditional collaborative filtering methods. Figure 2. User-item matrix decomposition \(R=\begin{bmatrix}5 &amp; 3 &amp; 0 &amp; 1 \\4 &amp; 0 &amp; 0&amp; 1\\1 &amp; 1 &amp; 0&amp; 5\\0 &amp; 1 &amp; 5&amp; 4\\\end{bmatrix}\rightarrow\;\;\;\; U \times V\;\;\rightarrow \;\;\hat{R}=\begin{bmatrix}4. 9&amp; 2. 5 &amp;2. 2 &amp;0. 9 \\4. 2 &amp; 0. 5 &amp; 3. 4&amp; 1\\1. 5 &amp; 0. 3 &amp; 4. 5&amp; 3. 9\\1. 1 &amp; 0. 9 &amp; 4. 9&amp; 3. 2\\\end{bmatrix}\)Rating matrix R can be expressed as the product of two lower-dimensional matrices: U (user matrix) and V (item matrix). Assume k=3 latent factors, which means we assume that there are three underlying factors that determine how users rate the items (e. g. action vs romance vs adventure, animation quality vs length vs vocal, etc). We initialize U and V randomly, then use gradient descent to optimize the matrices based on the mean squared error loss between the predicted ratings and actual ratings in R. We can then use these matrices to predict the missing values in R, by taking the dot product of the corresponding user and item vectors. B: Content-based Filtering workflowA way to understand content-based filtering is to see it as a classification problem, where the system identifies relevant features in the content that are highly correlated with the user’s preferences. Recommendations are then made by comparing the user’s profile to the content of each item in the collection. The inputs are descriptions of anime stories and the goal is to identify the key topics or themes present in the text by extracting and grouping related keywords. TF-IDF is a common method used for text extraction that calculates the importance of a specific word in a given document. Words that appear frequently across all documents, such as “people” or “place,” are given a lower importance than words that appear infrequently but suggest a specific theme or topic, such as “wolf,” “magic,” or “spirit”.  In this project, 30 different anime topics were identified by extracting the key phrases from all the story descriptions. For instance, Topic 7 is centered on solving crimes, with Detective Conan being the most representative example. Examples:  Topic #1: Special, release, air, recap, featureTopic #2: Earth, Planet, space, alien, shipTopic #3: High, school, junior, student, classmateTopic #4: Team, soccer, player, match, baseballTopic #5: Human, race, mankind, god, survive, extinct Topic #6: Magic, witch, magician, kingdom, wishTopic #7: Mystery, solve, appear, past, shadow, killTopic #8: Demon, king, hero, lord, seal, defeat, missionTopic #9: Love, feel, fall, relationship, confess, heart Then we can calculate an associated probability for each topic for a given anime.  For example       Anime/Topics   Topic 2   Topic 3   Topic 7   Topic 8   Topic 9         Fairy Tail   0. 01   0. 12   0. 00   0. 67   0. 00       Kimi no na wa   0. 11   0. 87   0. 02   0. 01   0. 54       No game no life   0. 35   0. 23   0. 34   0. 01   0. 35       Tokyo Ghoul   0. 02   0. 56   0. 76   0. 12   0. 00       One Piece   0. 05   0. 00   0. 00   0. 45   0. 00   We can determine a user’s preference for different topics by taking an average of all the anime topics they have watched in the past. To generate recommendations, we then calculate the cosine similarity between the user’s topic matrix and the anime topic matrix. Finally, we combine the results obtained from the two recommenders. Future Improvements: Incorporating contextual information, such as location, country, ethnicity, age to enhance the accuracy of recommendations by tailoring them to the user’s current situation. Use deep learning techniques, such as neural networks, to create more sophisticated models that can capture more complex patterns and relationships between users and items. Conclusion: Both methods have their strengths and limitations, and they can be combined to create a hybrid recommender system. The effectiveness of these methods can be further improved with advancements in natural language processing and machine learning techniques. Overall, recommender systems play a critical role in providing personalized experiences to users and are increasingly important in today’s digital landscape. Reference "
    }, {
    "id": 13,
    "url": "http://localhost:4000/illegal_mining/",
    "title": "Amazon Illegal Mining Detection",
    "body": "2019/01/18 - The Amazon rainforest, known for its rich biodiversity and vital role in regulating the Earth’s climate, has been facing a grave threat in the form of deforestation. This destructive practice has led to significant habitat destruction, loss of biodiversity, and contributes to climate change. A key factor exacerbating this devastation is the sharp increase in illegal gold mining activities observed in the region over the past decade. Motivated by the soaring global gold price, individuals have engaged in unlawful mining practices within the Amazon rainforest, further exacerbating the environmental crisis. Traditional labor-intensive methods involving the hiring of local citizens to spot illegal activities have proven to be costly and time-consuming. Fortunately, advancements in satellite technology have opened new avenues for detecting illegal mining activities using satellite images and employing machine learning techniques like image classification. The primary objective of this project is to develop a system capable of identifying illegal mining activities among 16 other labels, including clouds, trees, ground, road, water, and more. To achieve this, I employed a dataset obtained from “Planet: Understanding the Amazon from Space. ” The dataset consisted of satellite images that were segmented into smaller chips and randomly assigned numerical names. Our aim was to generate accurate labels that effectively describe the content of each image and enable the identification of those chips associated with illegal mining. Neural networks were chosen as the preferred approach for this project due to their exceptional ability to recognize complex patterns and extract meaningful features from images. Start with basic Neural Network Model:   The dataset consists of 70,000 image chips, each was associated with at least one label. The labels were categorized into three main groups:   atmospheric conditions,  common land uses and  rare land uses.  However, there were significant imbalances in the distribution of the labels, and a few labeling errors were present in the dataset. Among the labels, less than 1% represented categories such as artisanal mines, conventional mines, slash burn, and others. This highly imbalanced nature of the labels posed a challenge during the training process. The neural network had to learn to accurately classify and recognize the rare land use categories, despite having limited examples for these classes. I began by implementing a basic convolutional neural network (CNN) model. One of the key aspects is its ability to capture and extract important local features, allowing them to identify patterns and objects. However, after hours of training, the model struggled to predict any rare land use labels. The challenge here is how to predict the 1% population. Approaches I made to solve the problem:  Update computational power: use AWS GPU More advanced network designs Improve input image qualityUpdate computational powerTo harness the power of advanced hardware and accelerate the training process, an AWS GPU instance was utilized. The use of GPU computing combined with multi-processing technique allows for parallel processing and faster model training, enabling more efficient experimentation and optimization.  Additionally, cross-validation was implemented to continuously monitor the model’s performance across multiple iterations, ensuring that overfitting issues were identified and addressed promptly. Advanced network designs - DenseNetTo further improve the model’s performance and parameter efficiency, a DenseNet architecture was implemented. DenseNet is a deep neural network architecture that introduces dense connections between layers, allowing for direct information flow and gradient propagation throughout the network. This architecture alleviates the vanishing gradient problem and encourages feature reuse, leading to better parameter efficiency and faster convergence during training. As a result, the model benefits from requiring fewer parameters to achieve similar or even superior performance compared to CNN. This not only accelerates the training time but also helps mitigate the risk of overfitting by encouraging the model to learn more robust and generalized features from the data. Improve image qualitySatellite images often suffer from darkness and blurriness caused by atmospheric turbulence. However, by mitigating the effects of haze and improving image quality, we can enhance the performance of neural networks. Leveraging my understanding of atmospheric physics and conducting research on Google Scholars, I developed a dehaze function based on a paper to address this challenge. Haze results from the scattering of light in the atmosphere before it reaches the camera. To estimate the intensity of scattered light, a constant value is derived by approximating the maximum pixel intensity within the darkest RGB channel. By utilizing OpenCV to convert images to a colorspace matrix and calculating the haze constant, we can restore the image by subtracting this value. As a result, the dehazed image appears brighter and exhibits better contrast, improving the quality of input data for neural networks.  Figure 2. Formation of a hazy image. Camera = Direct attenuation+Airlight   Figure 3 showcases several examples of images before and after applying the haze removal function. The impact of the dehazing process is evident, as it leads to a substantial improvement in image quality across most samples. The “before” images appear hazy and lackluster, with reduced visibility and contrast. However, upon applying the haze removal function, the “after” images exhibit a remarkable transformation. The images become brighter, with enhanced contrast and sharper details. This dehazing technique proves particularly effective for images tagged as partly cloudy or affected by haze, successfully restoring their visual appeal and making them more suitable for analysis and interpretation. The results of the model demonstrate a notable enhancement in both precision and recall, particularly in the case of rare land use labels. Given the significance of these metrics and the preference for minimizing false negatives, the evaluation of the model’s performance is based on the F2 score. The F2 score is a composite metric that combines precision and recall, with a greater emphasis on recall. In scenarios where the consequences of missing positive instances are significant, such as detecting illegal activities or identifying rare occurrences, a higher recall is crucial. By assigning more weight to recall, the F2 score ensures that the model effectively identifies as many positive instances as possible, while still maintaining a reasonable level of precision.  Work Flow Model performanceAfter training the DenseNet model on the dehazed image sets, the model was saved as “b01_dense121. h5”. This trained model was then utilized to generate labels for the previously unlabelled test set images. The model successfully identified several test photo chips as illegal mines, providing valuable insights into the presence of illegal mining activities in the dataset.  Figure 4. Examples from the test data that were marked as illegal mines with labels generated by the DenseNet model. In summary, the project focused on developing a model to detect illegal mining and rare land use cases using satellite images. Several approaches were employed to address the challenges of imbalanced labels, low image quality, and underfitting. These approaches included improving computational power, rotating images of rare land use categories, using more advanced network designs like DenseNet, and enhancing input image quality through haze removal. The implementation of these approaches resulted in significant improvements in both precision and recall, particularly for the rare land use cases. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
<h3 class="font-weight-bold spanborder"><span>Contact</span></h3>
<div class="page-content">
<form action="https://formspree.io/mumuxi15@gmail.com" method="POST">    
<p class="mb-4">Please send your message to Penny Pan. We will reply as soon as possible!</p>
<div class="form-group row">
<div class="col-md-6">
<input class="form-control" type="text" name="name" placeholder="Name*" required="" />
</div>
<div class="col-md-6">
<input class="form-control" type="email" name="_replyto" placeholder="E-mail Address*" required="" />
</div>
</div>
<textarea rows="8" class="form-control mb-3" name="message" placeholder="Message*" required=""></textarea>    
<input class="btn btn-success" type="submit" value="Send" />
</form>

<!-- Comments -->

</div>
</div>
    </main>



    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Penny</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/mumuxi15/02-Data-Science-Project"><i class="fab fa-github"></i> Github Projects</a>

            </div>
            <div>
                Made with Jekyll Theme
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
